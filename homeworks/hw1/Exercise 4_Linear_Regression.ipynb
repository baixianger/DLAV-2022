{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !git clone https://github.com/vita-epfl/DLAV-2022.git\n",
    "# path = os.getcwd() + '/DLAV-2022/homeworks/hw1/data/ex1data1.txt'\n",
    "data = pd.read_csv(f\"data/ex1data1.txt\", header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (5, 1), array([1.00978044]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.rand(5)\n",
    "y1 = np.random.rand(5,1)\n",
    "x1.shape, y1.shape, x1@y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    # Compute the cost function\n",
    "    mse = np.mean(np.square(y - x @ theta)) / 2\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population   Profit\n",
       "0     1      6.1101  17.5920\n",
       "1     1      5.5277   9.1302\n",
       "2     1      8.5186  13.6620\n",
       "3     1      7.0032  11.8540\n",
       "4     1      5.8598   6.8233"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.072733877455676"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and loss\n",
    "    # 75*2 * \n",
    "    # ***************************************************\n",
    "    delta_mse = - tx.T @ (y - tx @ w) / len(y)\n",
    "    return delta_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = computeCost(X, y, theta)\n",
    "        gradient = compute_gradient(y, X, theta)   \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update theta by gradient\n",
    "        # ***************************************************\n",
    "        theta = theta - alpha * gradient\n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=32.072733877455676, w0=0.058391350515463895, w1=0.6532884974555669\n",
      "Gradient Descent(1/999): loss=6.737190464870009, w0=0.06289175271039382, w1=0.7700097825599364\n",
      "Gradient Descent(2/999): loss=5.9315935686049555, w0=0.057822927461428114, w1=0.7913481156584673\n",
      "Gradient Descent(3/999): loss=5.901154707081388, w0=0.051063625160778135, w1=0.795729810284954\n",
      "Gradient Descent(4/999): loss=5.895228586444221, w0=0.04401437836500259, w1=0.7970961782721866\n",
      "Gradient Descent(5/999): loss=5.8900949431173295, w0=0.03692413114216261, w1=0.7979254732843951\n",
      "Gradient Descent(6/999): loss=5.885004158443647, w0=0.029837117577144825, w1=0.7986582394519285\n",
      "Gradient Descent(7/999): loss=5.879932480491418, w0=0.022761181894038834, w1=0.7993727912003019\n",
      "Gradient Descent(8/999): loss=5.874879094762575, w0=0.01569769957420013, w1=0.8000830518518655\n",
      "Gradient Descent(9/999): loss=5.869843911806385, w0=0.008646896228913532, w1=0.8007914983590768\n",
      "Gradient Descent(10/999): loss=5.8648268653129305, w0=0.0016087930989843596, w1=0.8014985729280016\n",
      "Gradient Descent(11/999): loss=5.859827889932181, w0=-0.005416624870320648, w1=0.8022043560583255\n",
      "Gradient Descent(12/999): loss=5.85484692057229, w0=-0.012429379151800774, w1=0.8029088639482521\n",
      "Gradient Descent(13/999): loss=5.849883892376587, w0=-0.019429492325268326, w1=0.8036121013621461\n",
      "Gradient Descent(14/999): loss=5.844938740722034, w0=-0.026416987133500124, w1=0.8043140710284592\n",
      "Gradient Descent(15/999): loss=5.840011401218361, w0=-0.033391886314481416, w1=0.8050147753103407\n",
      "Gradient Descent(16/999): loss=5.835101809707226, w0=-0.04035421257164587, w1=0.8057142165026173\n",
      "Gradient Descent(17/999): loss=5.830209902261388, w0=-0.04730398856864606, w1=0.8064123968845912\n",
      "Gradient Descent(18/999): loss=5.825335615183863, w0=-0.05424123692848455, w1=0.8071093187294315\n",
      "Gradient Descent(19/999): loss=5.820478885007099, w0=-0.06116598023341994, w1=0.8078049843058496\n",
      "Gradient Descent(20/999): loss=5.815639648492154, w0=-0.06807824102501053, w1=0.8084993958784036\n",
      "Gradient Descent(21/999): loss=5.810817842627869, w0=-0.07497804180418248, w1=0.8091925557075582\n",
      "Gradient Descent(22/999): loss=5.806013404630044, w0=-0.08186540503130207, w1=0.8098844660497012\n",
      "Gradient Descent(23/999): loss=5.801226271940628, w0=-0.08874035312624866, w1=0.8105751291571527\n",
      "Gradient Descent(24/999): loss=5.7964563822269, w0=-0.0956029084684876, w1=0.8112645472781728\n",
      "Gradient Descent(25/999): loss=5.7917036733806535, w0=-0.10245309339714315, w1=0.8119527226569687\n",
      "Gradient Descent(26/999): loss=5.7869680835173956, w0=-0.10929093021107113, w1=0.8126396575337025\n",
      "Gradient Descent(27/999): loss=5.782249550975539, w0=-0.11611644116893155, w1=0.813325354144498\n",
      "Gradient Descent(28/999): loss=5.777548014315596, w0=-0.12292964848926106, w1=0.8140098147214483\n",
      "Gradient Descent(29/999): loss=5.772863412319381, w0=-0.12973057435054527, w1=0.8146930414926227\n",
      "Gradient Descent(30/999): loss=5.768195683989212, w0=-0.13651924089129092, w1=0.8153750366820742\n",
      "Gradient Descent(31/999): loss=5.76354476854712, w0=-0.14329567021009798, w1=0.8160558025098472\n",
      "Gradient Descent(32/999): loss=5.758910605434047, w0=-0.1500598843657316, w1=0.8167353411919838\n",
      "Gradient Descent(33/999): loss=5.754293134309077, w0=-0.15681190537719383, w1=0.8174136549405314\n",
      "Gradient Descent(34/999): loss=5.749692295048629, w0=-0.16355175522379548, w1=0.8180907459635505\n",
      "Gradient Descent(35/999): loss=5.745108027745684, w0=-0.17027945584522738, w1=0.8187666164651208\n",
      "Gradient Descent(36/999): loss=5.740540272709012, w0=-0.17699502914163212, w1=0.8194412686453493\n",
      "Gradient Descent(37/999): loss=5.735988970462381, w0=-0.1836984969736751, w1=0.8201147047003767\n",
      "Gradient Descent(38/999): loss=5.731454061743792, w0=-0.19038988116261576, w1=0.8207869268223856\n",
      "Gradient Descent(39/999): loss=5.726935487504702, w0=-0.1970692034903787, w1=0.8214579371996062\n",
      "Gradient Descent(40/999): loss=5.722433188909257, w0=-0.20373648569962446, w1=0.8221277380163249\n",
      "Gradient Descent(41/999): loss=5.717947107333529, w0=-0.2103917494938204, w1=0.8227963314528903\n",
      "Gradient Descent(42/999): loss=5.713477184364749, w0=-0.21703501653731122, w1=0.8234637196857207\n",
      "Gradient Descent(43/999): loss=5.709023361800549, w0=-0.22366630845538962, w1=0.8241299048873114\n",
      "Gradient Descent(44/999): loss=5.704585581648199, w0=-0.23028564683436664, w1=0.8247948892262416\n",
      "Gradient Descent(45/999): loss=5.700163786123857, w0=-0.23689305322164192, w1=0.8254586748671812\n",
      "Gradient Descent(46/999): loss=5.695757917651815, w0=-0.24348854912577383, w1=0.826121263970898\n",
      "Gradient Descent(47/999): loss=5.691367918863751, w0=-0.25007215601654953, w1=0.8267826586942654\n",
      "Gradient Descent(48/999): loss=5.68699373259798, w0=-0.25664389532505477, w1=0.8274428611902682\n",
      "Gradient Descent(49/999): loss=5.682635301898707, w0=-0.2632037884437438, w1=0.8281018736080105\n",
      "Gradient Descent(50/999): loss=5.678292570015292, w0=-0.2697518567265089, w1=0.8287596980927223\n",
      "Gradient Descent(51/999): loss=5.673965480401506, w0=-0.27628812148874987, w1=0.8294163367857669\n",
      "Gradient Descent(52/999): loss=5.669653976714796, w0=-0.28281260400744346, w1=0.8300717918246473\n",
      "Gradient Descent(53/999): loss=5.6653580028155535, w0=-0.2893253255212127, w1=0.8307260653430134\n",
      "Gradient Descent(54/999): loss=5.6610775027663784, w0=-0.2958263072303959, w1=0.8313791594706694\n",
      "Gradient Descent(55/999): loss=5.656812420831359, w0=-0.3023155702971157, w1=0.8320310763335801\n",
      "Gradient Descent(56/999): loss=5.6525627014753335, w0=-0.30879313584534807, w1=0.8326818180538778\n",
      "Gradient Descent(57/999): loss=5.648328289363181, w0=-0.315259024960991, w1=0.8333313867498697\n",
      "Gradient Descent(58/999): loss=5.644109129359092, w0=-0.32171325869193307, w1=0.8339797845360446\n",
      "Gradient Descent(59/999): loss=5.639905166525854, w0=-0.328155858048122, w1=0.8346270135230796\n",
      "Gradient Descent(60/999): loss=5.635716346124135, w0=-0.3345868440016331, w1=0.835273075817847\n",
      "Gradient Descent(61/999): loss=5.631542613611772, w0=-0.34100623748673753, w1=0.8359179735234217\n",
      "Gradient Descent(62/999): loss=5.627383914643055, w0=-0.3474140593999704, w1=0.8365617087390872\n",
      "Gradient Descent(63/999): loss=5.623240195068026, w0=-0.3538103306001988, w1=0.8372042835603428\n",
      "Gradient Descent(64/999): loss=5.619111400931778, w0=-0.3601950719086897, w1=0.8378457000789108\n",
      "Gradient Descent(65/999): loss=5.61499747847374, w0=-0.36656830410917784, w1=0.8384859603827427\n",
      "Gradient Descent(66/999): loss=5.610898374126984, w0=-0.37293004794793316, w1=0.8391250665560264\n",
      "Gradient Descent(67/999): loss=5.606814034517531, w0=-0.37928032413382856, w1=0.8397630206791926\n",
      "Gradient Descent(68/999): loss=5.602744406463646, w0=-0.38561915333840713, w1=0.8403998248289224\n",
      "Gradient Descent(69/999): loss=5.598689436975159, w0=-0.39194655619594954, w1=0.8410354810781528\n",
      "Gradient Descent(70/999): loss=5.59464907325276, w0=-0.3982625533035412, w1=0.8416699914960846\n",
      "Gradient Descent(71/999): loss=5.590623262687323, w0=-0.4045671652211394, w1=0.8423033581481884\n",
      "Gradient Descent(72/999): loss=5.586611952859217, w0=-0.41086041247163996, w1=0.8429355830962117\n",
      "Gradient Descent(73/999): loss=5.5826150915376225, w0=-0.41714231554094433, w1=0.8435666683981857\n",
      "Gradient Descent(74/999): loss=5.578632626679853, w0=-0.42341289487802614, w1=0.8441966161084314\n",
      "Gradient Descent(75/999): loss=5.574664506430678, w0=-0.42967217089499776, w1=0.844825428277567\n",
      "Gradient Descent(76/999): loss=5.570710679121643, w0=-0.43592016396717675, w1=0.8454531069525142\n",
      "Gradient Descent(77/999): loss=5.566771093270403, w0=-0.4421568944331523, w1=0.8460796541765048\n",
      "Gradient Descent(78/999): loss=5.562845697580051, w0=-0.4483823825948513, w1=0.8467050719890875\n",
      "Gradient Descent(79/999): loss=5.558934440938442, w0=-0.4545966487176044, w1=0.8473293624261348\n",
      "Gradient Descent(80/999): loss=5.555037272417543, w0=-0.4607997130302122, w1=0.8479525275198488\n",
      "Gradient Descent(81/999): loss=5.551154141272754, w0=-0.4669915957250108, w1=0.8485745692987691\n",
      "Gradient Descent(82/999): loss=5.547284996942256, w0=-0.47317231695793777, w1=0.8491954897877779\n",
      "Gradient Descent(83/999): loss=5.5434297890463515, w0=-0.47934189684859757, w1=0.8498152910081079\n",
      "Gradient Descent(84/999): loss=5.539588467386808, w0=-0.4855003554803273, w1=0.8504339749773481\n",
      "Gradient Descent(85/999): loss=5.535760981946204, w0=-0.4916477129002617, w1=0.8510515437094506\n",
      "Gradient Descent(86/999): loss=5.531947282887275, w0=-0.4977839891193989, w1=0.8516679992147372\n",
      "Gradient Descent(87/999): loss=5.5281473205522715, w0=-0.5039092041126652, w1=0.8522833434999061\n",
      "Gradient Descent(88/999): loss=5.524361045462306, w0=-0.5100233778189799, w1=0.8528975785680377\n",
      "Gradient Descent(89/999): loss=5.520588408316713, w0=-0.5161265301413209, w1=0.8535107064186023\n",
      "Gradient Descent(90/999): loss=5.5168293599924025, w0=-0.5222186809467889, w1=0.8541227290474656\n",
      "Gradient Descent(91/999): loss=5.513083851543225, w0=-0.5282998500666722, w1=0.8547336484468955\n",
      "Gradient Descent(92/999): loss=5.5093518341993315, w0=-0.5343700572965113, w1=0.8553434666055688\n",
      "Gradient Descent(93/999): loss=5.5056332593665385, w0=-0.5404293223961635, w1=0.8559521855085775\n",
      "Gradient Descent(94/999): loss=5.501928078625699, w0=-0.5464776650898668, w1=0.8565598071374354\n",
      "Gradient Descent(95/999): loss=5.498236243732065, w0=-0.5525151050663047, w1=0.8571663334700842\n",
      "Gradient Descent(96/999): loss=5.494557706614666, w0=-0.5585416619786697, w1=0.8577717664809001\n",
      "Gradient Descent(97/999): loss=5.490892419375677, w0=-0.5645573554447276, w1=0.8583761081407008\n",
      "Gradient Descent(98/999): loss=5.487240334289805, w0=-0.5705622050468814, w1=0.8589793604167507\n",
      "Gradient Descent(99/999): loss=5.483601403803652, w0=-0.5765562303322347, w1=0.8595815252727688\n",
      "Gradient Descent(100/999): loss=5.479975580535112, w0=-0.5825394508126558, w1=0.8601826046689336\n",
      "Gradient Descent(101/999): loss=5.476362817272741, w0=-0.5885118859648409, w1=0.8607826005618906\n",
      "Gradient Descent(102/999): loss=5.472763066975151, w0=-0.5944735552303778, w1=0.8613815149047582\n",
      "Gradient Descent(103/999): loss=5.469176282770398, w0=-0.6004244780158086, w1=0.861979349647134\n",
      "Gradient Descent(104/999): loss=5.465602417955358, w0=-0.6063646736926934, w1=0.8625761067351013\n",
      "Gradient Descent(105/999): loss=5.462041425995137, w0=-0.6122941615976734, w1=0.8631717881112356\n",
      "Gradient Descent(106/999): loss=5.4584932605224585, w0=-0.6182129610325333, w1=0.8637663957146104\n",
      "Gradient Descent(107/999): loss=5.454957875337047, w0=-0.6241210912642648, w1=0.864359931480804\n",
      "Gradient Descent(108/999): loss=5.451435224405051, w0=-0.630018571525129, w1=0.8649523973419058\n",
      "Gradient Descent(109/999): loss=5.4479252618584235, w0=-0.6359054210127186, w1=0.865543795226522\n",
      "Gradient Descent(110/999): loss=5.444427941994333, w0=-0.6417816588900213, w1=0.8661341270597828\n",
      "Gradient Descent(111/999): loss=5.440943219274565, w0=-0.6476473042854812, w1=0.8667233947633476\n",
      "Gradient Descent(112/999): loss=5.437471048324934, w0=-0.6535023762930622, w1=0.8673116002554123\n",
      "Gradient Descent(113/999): loss=5.434011383934687, w0=-0.6593468939723088, w1=0.8678987454507151\n",
      "Gradient Descent(114/999): loss=5.430564181055919, w0=-0.6651808763484093, w1=0.8684848322605423\n",
      "Gradient Descent(115/999): loss=5.427129394802985, w0=-0.671004342412257, w1=0.8690698625927351\n",
      "Gradient Descent(116/999): loss=5.423706980451918, w0=-0.6768173111205126, w1=0.8696538383516959\n",
      "Gradient Descent(117/999): loss=5.420296893439836, w0=-0.6826198013956652, w1=0.8702367614383937\n",
      "Gradient Descent(118/999): loss=5.416899089364382, w0=-0.6884118321260947, w1=0.8708186337503714\n",
      "Gradient Descent(119/999): loss=5.413513523983123, w0=-0.6941934221661327, w1=0.8713994571817509\n",
      "Gradient Descent(120/999): loss=5.410140153212988, w0=-0.6999645903361239, w1=0.8719792336232401\n",
      "Gradient Descent(121/999): loss=5.406778933129694, w0=-0.7057253554224879, w1=0.8725579649621386\n",
      "Gradient Descent(122/999): loss=5.403429819967165, w0=-0.7114757361777797, w1=0.873135653082344\n",
      "Gradient Descent(123/999): loss=5.400092770116975, w0=-0.7172157513207511, w1=0.8737122998643578\n",
      "Gradient Descent(124/999): loss=5.396767740127768, w0=-0.7229454195364116, w1=0.874287907185292\n",
      "Gradient Descent(125/999): loss=5.393454686704697, w0=-0.728664759476089, w1=0.8748624769188749\n",
      "Gradient Descent(126/999): loss=5.390153566708862, w0=-0.7343737897574906, w1=0.8754360109354568\n",
      "Gradient Descent(127/999): loss=5.386864337156746, w0=-0.7400725289647632, w1=0.876008511102017\n",
      "Gradient Descent(128/999): loss=5.383586955219661, w0=-0.745760995648554, w1=0.8765799792821695\n",
      "Gradient Descent(129/999): loss=5.380321378223178, w0=-0.751439208326071, w1=0.8771504173361683\n",
      "Gradient Descent(130/999): loss=5.37706756364658, w0=-0.7571071854811431, w1=0.8777198271209147\n",
      "Gradient Descent(131/999): loss=5.373825469122317, w0=-0.7627649455642801, w1=0.8782882104899624\n",
      "Gradient Descent(132/999): loss=5.37059505243543, w0=-0.7684125069927333, w1=0.8788555692935243\n",
      "Gradient Descent(133/999): loss=5.367376271523024, w0=-0.7740498881505551, w1=0.8794219053784776\n",
      "Gradient Descent(134/999): loss=5.364169084473712, w0=-0.7796771073886586, w1=0.8799872205883708\n",
      "Gradient Descent(135/999): loss=5.360973449527068, w0=-0.785294183024878, w1=0.8805515167634289\n",
      "Gradient Descent(136/999): loss=5.357789325073084, w0=-0.7909011333440276, w1=0.8811147957405598\n",
      "Gradient Descent(137/999): loss=5.354616669651632, w0=-0.7964979765979616, w1=0.8816770593533604\n",
      "Gradient Descent(138/999): loss=5.3514554419519165, w0=-0.8020847310056336, w1=0.882238309432122\n",
      "Gradient Descent(139/999): loss=5.348305600811943, w0=-0.8076614147531557, w1=0.8827985478038369\n",
      "Gradient Descent(140/999): loss=5.34516710521798, w0=-0.8132280459938577, w1=0.8833577762922041\n",
      "Gradient Descent(141/999): loss=5.342039914304029, w0=-0.8187846428483464, w1=0.883915996717635\n",
      "Gradient Descent(142/999): loss=5.338923987351284, w0=-0.8243312234045646, w1=0.8844732108972596\n",
      "Gradient Descent(143/999): loss=5.335819283787603, w0=-0.8298678057178496, w1=0.8850294206449325\n",
      "Gradient Descent(144/999): loss=5.332725763186988, w0=-0.8353944078109924, w1=0.8855846277712385\n",
      "Gradient Descent(145/999): loss=5.329643385269053, w0=-0.840911047674296, w1=0.8861388340834986\n",
      "Gradient Descent(146/999): loss=5.326572109898499, w0=-0.8464177432656345, w1=0.8866920413857761\n",
      "Gradient Descent(147/999): loss=5.323511897084587, w0=-0.8519145125105108, w1=0.8872442514788821\n",
      "Gradient Descent(148/999): loss=5.32046270698063, w0=-0.8574013733021155, w1=0.887795466160382\n",
      "Gradient Descent(149/999): loss=5.317424499883461, w0=-0.8628783435013853, w1=0.8883456872246002\n",
      "Gradient Descent(150/999): loss=5.314397236232924, w0=-0.8683454409370605, w1=0.8888949164626272\n",
      "Gradient Descent(151/999): loss=5.311380876611354, w0=-0.8738026834057434, w1=0.8894431556623249\n",
      "Gradient Descent(152/999): loss=5.3083753817430726, w0=-0.8792500886719564, w1=0.8899904066083322\n",
      "Gradient Descent(153/999): loss=5.305380712493861, w0=-0.8846876744681996, w1=0.8905366710820709\n",
      "Gradient Descent(154/999): loss=5.302396829870465, w0=-0.8901154584950085, w1=0.891081950861752\n",
      "Gradient Descent(155/999): loss=5.2994236950200815, w0=-0.8955334584210117, w1=0.8916262477223809\n",
      "Gradient Descent(156/999): loss=5.296461269229852, w0=-0.9009416918829886, w1=0.892169563435763\n",
      "Gradient Descent(157/999): loss=5.29350951392636, w0=-0.9063401764859261, w1=0.8927118997705107\n",
      "Gradient Descent(158/999): loss=5.290568390675129, w0=-0.9117289298030771, w1=0.8932532584920476\n",
      "Gradient Descent(159/999): loss=5.287637861180118, w0=-0.9171079693760165, w1=0.8937936413626153\n",
      "Gradient Descent(160/999): loss=5.284717887283231, w0=-0.922477312714699, w1=0.8943330501412785\n",
      "Gradient Descent(161/999): loss=5.281808430963811, w0=-0.9278369772975161, w1=0.8948714865839315\n",
      "Gradient Descent(162/999): loss=5.278909454338152, w0=-0.9331869805713527, w1=0.8954089524433027\n",
      "Gradient Descent(163/999): loss=5.276020919659, w0=-0.938527339951644, w1=0.8959454494689618\n",
      "Gradient Descent(164/999): loss=5.27314278931507, w0=-0.943858072822432, w1=0.8964809794073243\n",
      "Gradient Descent(165/999): loss=5.270275025830544, w0=-0.9491791965364226, w1=0.8970155440016577\n",
      "Gradient Descent(166/999): loss=5.267417591864593, w0=-0.9544907284150417, w1=0.8975491449920872\n",
      "Gradient Descent(167/999): loss=5.264570450210885, w0=-0.9597926857484916, w1=0.898081784115601\n",
      "Gradient Descent(168/999): loss=5.261733563797111, w0=-0.9650850857958077, w1=0.8986134631060566\n",
      "Gradient Descent(169/999): loss=5.2589068956844836, w0=-0.9703679457849137, w1=0.8991441836941857\n",
      "Gradient Descent(170/999): loss=5.256090409067273, w0=-0.9756412829126788, w1=0.8996739476076004\n",
      "Gradient Descent(171/999): loss=5.253284067272322, w0=-0.9809051143449731, w1=0.9002027565707984\n",
      "Gradient Descent(172/999): loss=5.250487833758564, w0=-0.9861594572167234, w1=0.9007306123051692\n",
      "Gradient Descent(173/999): loss=5.2477016721165555, w0=-0.9914043286319694, w1=0.9012575165289988\n",
      "Gradient Descent(174/999): loss=5.244925546067995, w0=-0.996639745663919, w1=0.9017834709574764\n",
      "Gradient Descent(175/999): loss=5.242159419465253, w0=-1.001865725355004, w1=0.902308477302699\n",
      "Gradient Descent(176/999): loss=5.2394032562909025, w0=-1.0070822847169358, w1=0.9028325372736775\n",
      "Gradient Descent(177/999): loss=5.236657020657251, w0=-1.01228944073076, w1=0.9033556525763423\n",
      "Gradient Descent(178/999): loss=5.2339206768058695, w0=-1.017487210346913, w1=0.9038778249135484\n",
      "Gradient Descent(179/999): loss=5.2311941891071285, w0=-1.0226756104852757, w1=0.9043990559850815\n",
      "Gradient Descent(180/999): loss=5.228477522059736, w0=-1.0278546580352297, w1=0.9049193474876631\n",
      "Gradient Descent(181/999): loss=5.225770640290271, w0=-1.0330243698557118, w1=0.9054387011149563\n",
      "Gradient Descent(182/999): loss=5.223073508552729, w0=-1.038184762775269, w1=0.9059571185575711\n",
      "Gradient Descent(183/999): loss=5.220386091728056, w0=-1.0433358535921131, w1=0.9064746015030702\n",
      "Gradient Descent(184/999): loss=5.217708354823696, w0=-1.0484776590741756, w1=0.9069911516359741\n",
      "Gradient Descent(185/999): loss=5.215040262973137, w0=-1.0536101959591622, w1=0.907506770637767\n",
      "Gradient Descent(186/999): loss=5.212381781435449, w0=-1.0587334809546072, w1=0.9080214601869019\n",
      "Gradient Descent(187/999): loss=5.2097328755948435, w0=-1.063847530737928, w1=0.9085352219588062\n",
      "Gradient Descent(188/999): loss=5.207093510960208, w0=-1.0689523619564796, w1=0.9090480576258873\n",
      "Gradient Descent(189/999): loss=5.204463653164672, w0=-1.074047991227608, w1=0.9095599688575379\n",
      "Gradient Descent(190/999): loss=5.201843267965149, w0=-1.0791344351387053, w1=0.9100709573201414\n",
      "Gradient Descent(191/999): loss=5.199232321241896, w0=-1.0842117102472633, w1=0.9105810246770776\n",
      "Gradient Descent(192/999): loss=5.1966307789980615, w0=-1.0892798330809268, w1=0.9110901725887276\n",
      "Gradient Descent(193/999): loss=5.194038607359259, w0=-1.0943388201375486, w1=0.9115984027124796\n",
      "Gradient Descent(194/999): loss=5.191455772573107, w0=-1.0993886878852421, w1=0.9121057167027342\n",
      "Gradient Descent(195/999): loss=5.188882241008802, w0=-1.1044294527624354, w1=0.91261211621091\n",
      "Gradient Descent(196/999): loss=5.1863179791566765, w0=-1.109461131177925, w1=0.9131176028854483\n",
      "Gradient Descent(197/999): loss=5.18376295362776, w0=-1.1144837395109286, w1=0.9136221783718195\n",
      "Gradient Descent(198/999): loss=5.181217131153349, w0=-1.1194972941111392, w1=0.9141258443125273\n",
      "Gradient Descent(199/999): loss=5.1786804785845755, w0=-1.1245018112987775, w1=0.9146286023471152\n",
      "Gradient Descent(200/999): loss=5.176152962891967, w0=-1.1294973073646457, w1=0.9151304541121706\n",
      "Gradient Descent(201/999): loss=5.173634551165022, w0=-1.1344837985701803, w1=0.9156314012413316\n",
      "Gradient Descent(202/999): loss=5.171125210611782, w0=-1.1394613011475048, w1=0.9161314453652908\n",
      "Gradient Descent(203/999): loss=5.168624908558404, w0=-1.1444298312994827, w1=0.9166305881118018\n",
      "Gradient Descent(204/999): loss=5.166133612448731, w0=-1.1493894051997708, w1=0.9171288311056836\n",
      "Gradient Descent(205/999): loss=5.163651289843875, w0=-1.1543400389928709, w1=0.9176261759688266\n",
      "Gradient Descent(206/999): loss=5.161177908421789, w0=-1.1592817487941824, w1=0.9181226243201975\n",
      "Gradient Descent(207/999): loss=5.15871343597685, w0=-1.164214550690056, w1=0.9186181777758449\n",
      "Gradient Descent(208/999): loss=5.156257840419434, w0=-1.169138460737845, w1=0.9191128379489037\n",
      "Gradient Descent(209/999): loss=5.153811089775505, w0=-1.1740534949659573, w1=0.9196066064496018\n",
      "Gradient Descent(210/999): loss=5.151373152186197, w0=-1.1789596693739084, w1=0.9200994848852637\n",
      "Gradient Descent(211/999): loss=5.148943995907395, w0=-1.1838569999323731, w1=0.9205914748603171\n",
      "Gradient Descent(212/999): loss=5.146523589309322, w0=-1.1887455025832376, w1=0.9210825779762974\n",
      "Gradient Descent(213/999): loss=5.144111900876139, w0=-1.1936251932396513, w1=0.921572795831853\n",
      "Gradient Descent(214/999): loss=5.141708899205515, w0=-1.1984960877860784, w1=0.9220621300227506\n",
      "Gradient Descent(215/999): loss=5.139314553008234, w0=-1.2033582020783502, w1=0.9225505821418805\n",
      "Gradient Descent(216/999): loss=5.136928831107778, w0=-1.208211551943716, w1=0.9230381537792615\n",
      "Gradient Descent(217/999): loss=5.134551702439933, w0=-1.213056153180895, w1=0.9235248465220464\n",
      "Gradient Descent(218/999): loss=5.1321831360523635, w0=-1.217892021560128, w1=0.9240106619545269\n",
      "Gradient Descent(219/999): loss=5.129823101104237, w0=-1.2227191728232283, w1=0.9244956016581389\n",
      "Gradient Descent(220/999): loss=5.127471566865799, w0=-1.227537622683633, w1=0.9249796672114676\n",
      "Gradient Descent(221/999): loss=5.12512850271798, w0=-1.232347386826454, w1=0.9254628601902526\n",
      "Gradient Descent(222/999): loss=5.122793878152007, w0=-1.2371484809085298, w1=0.9259451821673932\n",
      "Gradient Descent(223/999): loss=5.120467662768992, w0=-1.2419409205584755, w1=0.9264266347129532\n",
      "Gradient Descent(224/999): loss=5.118149826279542, w0=-1.2467247213767343, w1=0.9269072193941664\n",
      "Gradient Descent(225/999): loss=5.115840338503367, w0=-1.2514998989356283, w1=0.9273869377754415\n",
      "Gradient Descent(226/999): loss=5.113539169368884, w0=-1.2562664687794085, w1=0.9278657914183671\n",
      "Gradient Descent(227/999): loss=5.111246288912825, w0=-1.2610244464243063, w1=0.928343781881717\n",
      "Gradient Descent(228/999): loss=5.108961667279848, w0=-1.2657738473585838, w1=0.928820910721455\n",
      "Gradient Descent(229/999): loss=5.10668527472215, w0=-1.2705146870425834, w1=0.9292971794907403\n",
      "Gradient Descent(230/999): loss=5.104417081599077, w0=-1.2752469809087792, w1=0.9297725897399323\n",
      "Gradient Descent(231/999): loss=5.102157058376736, w0=-1.2799707443618265, w1=0.9302471430165956\n",
      "Gradient Descent(232/999): loss=5.099905175627619, w0=-1.2846859927786125, w1=0.9307208408655054\n",
      "Gradient Descent(233/999): loss=5.097661404030211, w0=-1.289392741508306, w1=0.9311936848286524\n",
      "Gradient Descent(234/999): loss=5.09542571436861, w0=-1.2940910058724073, w1=0.9316656764452471\n",
      "Gradient Descent(235/999): loss=5.093198077532151, w0=-1.2987808011647985, w1=0.932136817251726\n",
      "Gradient Descent(236/999): loss=5.090978464515021, w0=-1.303462142651793, w1=0.932607108781756\n",
      "Gradient Descent(237/999): loss=5.088766846415887, w0=-1.3081350455721847, w1=0.9330765525662391\n",
      "Gradient Descent(238/999): loss=5.086563194437517, w0=-1.312799525137299, w1=0.9335451501333181\n",
      "Gradient Descent(239/999): loss=5.084367479886401, w0=-1.3174555965310406, w1=0.9340129030083807\n",
      "Gradient Descent(240/999): loss=5.082179674172386, w0=-1.3221032749099442, w1=0.9344798127140652\n",
      "Gradient Descent(241/999): loss=5.079999748808297, w0=-1.3267425754032232, w1=0.9349458807702655\n",
      "Gradient Descent(242/999): loss=5.0778276754095675, w0=-1.331373513112819, w1=0.9354111086941351\n",
      "Gradient Descent(243/999): loss=5.075663425693872, w0=-1.335996103113451, w1=0.9358754980000933\n",
      "Gradient Descent(244/999): loss=5.073506971480756, w0=-1.3406103604526642, w1=0.9363390501998291\n",
      "Gradient Descent(245/999): loss=5.071358284691268, w0=-1.3452163001508792, w1=0.9368017668023066\n",
      "Gradient Descent(246/999): loss=5.069217337347596, w0=-1.349813937201441, w1=0.9372636493137702\n",
      "Gradient Descent(247/999): loss=5.067084101572705, w0=-1.3544032865706677, w1=0.9377246992377486\n",
      "Gradient Descent(248/999): loss=5.064958549589969, w0=-1.3589843631978988, w1=0.9381849180750605\n",
      "Gradient Descent(249/999): loss=5.06284065372281, w0=-1.3635571819955448, w1=0.9386443073238195\n",
      "Gradient Descent(250/999): loss=5.060730386394342, w0=-1.3681217578491345, w1=0.9391028684794385\n",
      "Gradient Descent(251/999): loss=5.05862772012701, w0=-1.3726781056173645, w1=0.9395606030346343\n",
      "Gradient Descent(252/999): loss=5.056532627542231, w0=-1.377226240132147, w1=0.9400175124794338\n",
      "Gradient Descent(253/999): loss=5.054445081360036, w0=-1.3817661761986584, w1=0.9404735983011773\n",
      "Gradient Descent(254/999): loss=5.052365054398719, w0=-1.3862979285953874, w1=0.9409288619845244\n",
      "Gradient Descent(255/999): loss=5.050292519574479, w0=-1.3908215120741827, w1=0.9413833050114583\n",
      "Gradient Descent(256/999): loss=5.0482274499010735, w0=-1.3953369413603018, w1=0.9418369288612908\n",
      "Gradient Descent(257/999): loss=5.046169818489457, w0=-1.3998442311524586, w1=0.9422897350106672\n",
      "Gradient Descent(258/999): loss=5.044119598547441, w0=-1.4043433961228704, w1=0.9427417249335708\n",
      "Gradient Descent(259/999): loss=5.042076763379341, w0=-1.4088344509173072, w1=0.9431929001013282\n",
      "Gradient Descent(260/999): loss=5.040041286385627, w0=-1.4133174101551385, w1=0.9436432619826134\n",
      "Gradient Descent(261/999): loss=5.038013141062576, w0=-1.4177922884293805, w1=0.9440928120434534\n",
      "Gradient Descent(262/999): loss=5.035992301001939, w0=-1.4222591003067446, w1=0.9445415517472323\n",
      "Gradient Descent(263/999): loss=5.033978739890578, w0=-1.426717860327684, w1=0.9449894825546963\n",
      "Gradient Descent(264/999): loss=5.031972431510141, w0=-1.4311685830064413, w1=0.9454366059239585\n",
      "Gradient Descent(265/999): loss=5.029973349736707, w0=-1.435611282831096, w1=0.9458829233105037\n",
      "Gradient Descent(266/999): loss=5.027981468540455, w0=-1.4400459742636116, w1=0.9463284361671929\n",
      "Gradient Descent(267/999): loss=5.025996761985324, w0=-1.4444726717398821, w1=0.9467731459442682\n",
      "Gradient Descent(268/999): loss=5.024019204228667, w0=-1.4488913896697797, w1=0.9472170540893577\n",
      "Gradient Descent(269/999): loss=5.022048769520927, w0=-1.4533021424372015, w1=0.9476601620474799\n",
      "Gradient Descent(270/999): loss=5.020085432205293, w0=-1.4577049444001158, w1=0.9481024712610484\n",
      "Gradient Descent(271/999): loss=5.018129166717367, w0=-1.4620998098906097, w1=0.9485439831698768\n",
      "Gradient Descent(272/999): loss=5.016179947584834, w0=-1.4664867532149353, w1=0.9489846992111832\n",
      "Gradient Descent(273/999): loss=5.014237749427129, w0=-1.470865788653556, w1=0.9494246208195952\n",
      "Gradient Descent(274/999): loss=5.012302546955106, w0=-1.4752369304611939, w1=0.9498637494271542\n",
      "Gradient Descent(275/999): loss=5.010374314970709, w0=-1.479600192866875, w1=0.9503020864633202\n",
      "Gradient Descent(276/999): loss=5.008453028366642, w0=-1.4839555900739763, w1=0.9507396333549765\n",
      "Gradient Descent(277/999): loss=5.006538662126046, w0=-1.4883031362602719, w1=0.9511763915264342\n",
      "Gradient Descent(278/999): loss=5.004631191322175, w0=-1.4926428455779792, w1=0.9516123623994371\n",
      "Gradient Descent(279/999): loss=5.002730591118061, w0=-1.4969747321538047, w1=0.9520475473931662\n",
      "Gradient Descent(280/999): loss=5.0008368367662, w0=-1.5012988100889904, w1=0.9524819479242441\n",
      "Gradient Descent(281/999): loss=4.998949903608226, w0=-1.505615093459359, w1=0.9529155654067399\n",
      "Gradient Descent(282/999): loss=4.997069767074595, w0=-1.5099235963153608, w1=0.9533484012521737\n",
      "Gradient Descent(283/999): loss=4.995196402684257, w0=-1.514224332682118, w1=0.9537804568695212\n",
      "Gradient Descent(284/999): loss=4.993329786044343, w0=-1.518517316559472, w1=0.9542117336652183\n",
      "Gradient Descent(285/999): loss=4.991469892849846, w0=-1.522802561922028, w1=0.9546422330431656\n",
      "Gradient Descent(286/999): loss=4.9896166988833, w0=-1.5270800827192001, w1=0.9550719564047332\n",
      "Gradient Descent(287/999): loss=4.987770180014478, w0=-1.5313498928752576, w1=0.955500905148765\n",
      "Gradient Descent(288/999): loss=4.98593031220006, w0=-1.5356120062893701, w1=0.9559290806715831\n",
      "Gradient Descent(289/999): loss=4.984097071483333, w0=-1.5398664368356523, w1=0.956356484366993\n",
      "Gradient Descent(290/999): loss=4.982270433993872, w0=-1.5441131983632097, w1=0.9567831176262876\n",
      "Gradient Descent(291/999): loss=4.9804503759472345, w0=-1.5483523046961836, w1=0.9572089818382518\n",
      "Gradient Descent(292/999): loss=4.978636873644648, w0=-1.5525837696337954, w1=0.957634078389167\n",
      "Gradient Descent(293/999): loss=4.9768299034727, w0=-1.5568076069503929, w1=0.9580584086628159\n",
      "Gradient Descent(294/999): loss=4.975029441903031, w0=-1.5610238303954935, w1=0.9584819740404867\n",
      "Gradient Descent(295/999): loss=4.973235465492034, w0=-1.5652324536938302, w1=0.9589047759009777\n",
      "Gradient Descent(296/999): loss=4.971447950880541, w0=-1.569433490545396, w1=0.9593268156206016\n",
      "Gradient Descent(297/999): loss=4.969666874793521, w0=-1.573626954625488, w1=0.9597480945731905\n",
      "Gradient Descent(298/999): loss=4.967892214039785, w0=-1.5778128595847523, w1=0.9601686141300999\n",
      "Gradient Descent(299/999): loss=4.966123945511668, w0=-1.5819912190492287, w1=0.9605883756602133\n",
      "Gradient Descent(300/999): loss=4.964362046184744, w0=-1.5861620466203945, w1=0.9610073805299465\n",
      "Gradient Descent(301/999): loss=4.962606493117519, w0=-1.5903253558752093, w1=0.9614256301032524\n",
      "Gradient Descent(302/999): loss=4.96085726345113, w0=-1.5944811603661584, w1=0.9618431257416252\n",
      "Gradient Descent(303/999): loss=4.959114334409053, w0=-1.598629473621298, w1=0.9622598688041049\n",
      "Gradient Descent(304/999): loss=4.957377683296804, w0=-1.6027703091442984, w1=0.9626758606472816\n",
      "Gradient Descent(305/999): loss=4.95564728750164, w0=-1.6069036804144885, w1=0.9630911026253001\n",
      "Gradient Descent(306/999): loss=4.95392312449227, w0=-1.611029600886899, w1=0.9635055960898642\n",
      "Gradient Descent(307/999): loss=4.95220517181856, w0=-1.6151480839923067, w1=0.963919342390241\n",
      "Gradient Descent(308/999): loss=4.95049340711124, w0=-1.6192591431372787, w1=0.9643323428732659\n",
      "Gradient Descent(309/999): loss=4.948787808081611, w0=-1.6233627917042148, w1=0.9647445988833457\n",
      "Gradient Descent(310/999): loss=4.9470883525212574, w0=-1.627459043051392, w1=0.9651561117624645\n",
      "Gradient Descent(311/999): loss=4.94539501830176, w0=-1.6315479105130077, w1=0.965566882850187\n",
      "Gradient Descent(312/999): loss=4.943707783374399, w0=-1.6356294073992232, w1=0.9659769134836632\n",
      "Gradient Descent(313/999): loss=4.942026625769877, w0=-1.6397035469962071, w1=0.966386204997633\n",
      "Gradient Descent(314/999): loss=4.940351523598029, w0=-1.643770342566178, w1=0.9667947587244299\n",
      "Gradient Descent(315/999): loss=4.938682455047537, w0=-1.6478298073474484, w1=0.9672025759939861\n",
      "Gradient Descent(316/999): loss=4.937019398385641, w0=-1.6518819545544674, w1=0.9676096581338364\n",
      "Gradient Descent(317/999): loss=4.935362331957869, w0=-1.6559267973778635, w1=0.9680160064691224\n",
      "Gradient Descent(318/999): loss=4.933711234187743, w0=-1.6599643489844884, w1=0.9684216223225973\n",
      "Gradient Descent(319/999): loss=4.9320660835764984, w0=-1.6639946225174589, w1=0.9688265070146297\n",
      "Gradient Descent(320/999): loss=4.930426858702819, w0=-1.6680176310962, w1=0.969230661863208\n",
      "Gradient Descent(321/999): loss=4.928793538222535, w0=-1.6720333878164881, w1=0.9696340881839453\n",
      "Gradient Descent(322/999): loss=4.927166100868362, w0=-1.676041905750493, w1=0.9700367872900825\n",
      "Gradient Descent(323/999): loss=4.925544525449623, w0=-1.6800431979468202, w1=0.9704387604924937\n",
      "Gradient Descent(324/999): loss=4.923928790851962, w0=-1.6840372774305545, w1=0.9708400090996899\n",
      "Gradient Descent(325/999): loss=4.922318876037079, w0=-1.6880241572033017, w1=0.9712405344178237\n",
      "Gradient Descent(326/999): loss=4.920714760042454, w0=-1.6920038502432304, w1=0.9716403377506925\n",
      "Gradient Descent(327/999): loss=4.919116421981069, w0=-1.695976369505115, w1=0.9720394203997442\n",
      "Gradient Descent(328/999): loss=4.91752384104114, w0=-1.6999417279203783, w1=0.9724377836640803\n",
      "Gradient Descent(329/999): loss=4.915936996485852, w0=-1.7038999383971323, w1=0.9728354288404608\n",
      "Gradient Descent(330/999): loss=4.914355867653075, w0=-1.707851013820221, w1=0.973232357223308\n",
      "Gradient Descent(331/999): loss=4.912780433955111, w0=-1.7117949670512624, w1=0.973628570104711\n",
      "Gradient Descent(332/999): loss=4.911210674878409, w0=-1.71573181092869, w1=0.9740240687744296\n",
      "Gradient Descent(333/999): loss=4.9096465699833125, w0=-1.7196615582677952, w1=0.9744188545198987\n",
      "Gradient Descent(334/999): loss=4.908088098903784, w0=-1.723584221860768, w1=0.9748129286262328\n",
      "Gradient Descent(335/999): loss=4.906535241347148, w0=-1.7274998144767397, w1=0.9752062923762296\n",
      "Gradient Descent(336/999): loss=4.904987977093815, w0=-1.731408348861824, w1=0.9755989470503743\n",
      "Gradient Descent(337/999): loss=4.903446285997033, w0=-1.7353098377391583, w1=0.9759908939268442\n",
      "Gradient Descent(338/999): loss=4.90191014798261, w0=-1.7392042938089454, w1=0.9763821342815124\n",
      "Gradient Descent(339/999): loss=4.9003795430486665, w0=-1.743091729748495, w1=0.9767726693879522\n",
      "Gradient Descent(340/999): loss=4.898854451265366, w0=-1.7469721582122641, w1=0.977162500517441\n",
      "Gradient Descent(341/999): loss=4.897334852774656, w0=-1.7508455918318997, w1=0.9775516289389647\n",
      "Gradient Descent(342/999): loss=4.895820727790016, w0=-1.7547120432162784, w1=0.977940055919222\n",
      "Gradient Descent(343/999): loss=4.894312056596192, w0=-1.7585715249515483, w1=0.9783277827226277\n",
      "Gradient Descent(344/999): loss=4.892808819548944, w0=-1.7624240496011698, w1=0.9787148106113178\n",
      "Gradient Descent(345/999): loss=4.89131099707479, w0=-1.7662696297059566, w1=0.979101140845153\n",
      "Gradient Descent(346/999): loss=4.889818569670749, w0=-1.770108277784116, w1=0.979486774681723\n",
      "Gradient Descent(347/999): loss=4.88833151790409, w0=-1.77394000633129, w1=0.9798717133763506\n",
      "Gradient Descent(348/999): loss=4.88684982241208, w0=-1.7777648278205966, w1=0.980255958182096\n",
      "Gradient Descent(349/999): loss=4.885373463901725, w0=-1.7815827547026695, w1=0.98063951034976\n",
      "Gradient Descent(350/999): loss=4.883902423149523, w0=-1.7853937994056985, w1=0.9810223711278895\n",
      "Gradient Descent(351/999): loss=4.88243668100122, w0=-1.7891979743354711, w1=0.98140454176278\n",
      "Gradient Descent(352/999): loss=4.880976218371547, w0=-1.7929952918754117, w1=0.9817860234984812\n",
      "Gradient Descent(353/999): loss=4.879521016243985, w0=-1.7967857643866227, w1=0.9821668175767997\n",
      "Gradient Descent(354/999): loss=4.878071055670509, w0=-1.8005694042079243, w1=0.9825469252373038\n",
      "Gradient Descent(355/999): loss=4.876626317771341, w0=-1.8043462236558947, w1=0.9829263477173273\n",
      "Gradient Descent(356/999): loss=4.875186783734713, w0=-1.8081162350249103, w1=0.983305086251974\n",
      "Gradient Descent(357/999): loss=4.87375243481661, w0=-1.8118794505871858, w1=0.9836831420741207\n",
      "Gradient Descent(358/999): loss=4.872323252340535, w0=-1.815635882592814, w1=0.9840605164144222\n",
      "Gradient Descent(359/999): loss=4.8708992176972625, w0=-1.819385543269806, w1=0.9844372105013148\n",
      "Gradient Descent(360/999): loss=4.869480312344593, w0=-1.8231284448241303, w1=0.9848132255610205\n",
      "Gradient Descent(361/999): loss=4.868066517807122, w0=-1.8268645994397532, w1=0.9851885628175507\n",
      "Gradient Descent(362/999): loss=4.866657815675987, w0=-1.8305940192786783, w1=0.9855632234927107\n",
      "Gradient Descent(363/999): loss=4.865254187608633, w0=-1.8343167164809857, w1=0.9859372088061034\n",
      "Gradient Descent(364/999): loss=4.863855615328574, w0=-1.8380327031648724, w1=0.9863105199751329\n",
      "Gradient Descent(365/999): loss=4.862462080625159, w0=-1.8417419914266906, w1=0.986683158215009\n",
      "Gradient Descent(366/999): loss=4.861073565353324, w0=-1.845444593340988, w1=0.9870551247387511\n",
      "Gradient Descent(367/999): loss=4.859690051433372, w0=-1.8491405209605467, w1=0.9874264207571918\n",
      "Gradient Descent(368/999): loss=4.858311520850714, w0=-1.8528297863164227, w1=0.9877970474789811\n",
      "Gradient Descent(369/999): loss=4.856937955655664, w0=-1.8565124014179843, w1=0.9881670061105906\n",
      "Gradient Descent(370/999): loss=4.855569337963178, w0=-1.8601883782529525, w1=0.9885362978563166\n",
      "Gradient Descent(371/999): loss=4.854205649952639, w0=-1.8638577287874387, w1=0.988904923918285\n",
      "Gradient Descent(372/999): loss=4.852846873867617, w0=-1.8675204649659847, w1=0.9892728854964544\n",
      "Gradient Descent(373/999): loss=4.85149299201564, w0=-1.8711765987116007, w1=0.9896401837886206\n",
      "Gradient Descent(374/999): loss=4.850143986767962, w0=-1.8748261419258045, w1=0.9900068199904203\n",
      "Gradient Descent(375/999): loss=4.8487998405593355, w0=-1.878469106488661, w1=0.9903727952953345\n",
      "Gradient Descent(376/999): loss=4.847460535887785, w0=-1.882105504258819, w1=0.9907381108946934\n",
      "Gradient Descent(377/999): loss=4.84612605531437, w0=-1.885735347073552, w1=0.9911027679776796\n",
      "Gradient Descent(378/999): loss=4.844796381462969, w0=-1.8893586467487953, w1=0.9914667677313317\n",
      "Gradient Descent(379/999): loss=4.8434714970200465, w0=-1.8929754150791847, w1=0.9918301113405489\n",
      "Gradient Descent(380/999): loss=4.842151384734428, w0=-1.896585663838095, w1=0.9921927999880944\n",
      "Gradient Descent(381/999): loss=4.840836027417081, w0=-1.9001894047776786, w1=0.9925548348545994\n",
      "Gradient Descent(382/999): loss=4.839525407940883, w0=-1.9037866496289035, w1=0.9929162171185668\n",
      "Gradient Descent(383/999): loss=4.838219509240404, w0=-1.9073774101015915, w1=0.9932769479563754\n",
      "Gradient Descent(384/999): loss=4.836918314311682, w0=-1.910961697884456, w1=0.9936370285422832\n",
      "Gradient Descent(385/999): loss=4.835621806212003, w0=-1.9145395246451407, w1=0.9939964600484313\n",
      "Gradient Descent(386/999): loss=4.834329968059677, w0=-1.9181109020302571, w1=0.9943552436448486\n",
      "Gradient Descent(387/999): loss=4.833042783033826, w0=-1.921675841665423, w1=0.9947133804994541\n",
      "Gradient Descent(388/999): loss=4.8317602343741575, w0=-1.9252343551552993, w1=0.9950708717780621\n",
      "Gradient Descent(389/999): loss=4.830482305380745, w0=-1.9287864540836286, w1=0.9954277186443851\n",
      "Gradient Descent(390/999): loss=4.829208979413817, w0=-1.9323321500132729, w1=0.995783922260038\n",
      "Gradient Descent(391/999): loss=4.827940239893541, w0=-1.9358714544862508, w1=0.9961394837845419\n",
      "Gradient Descent(392/999): loss=4.826676070299799, w0=-1.9394043790237754, w1=0.9964944043753273\n",
      "Gradient Descent(393/999): loss=4.825416454171979, w0=-1.9429309351262916, w1=0.9968486851877391\n",
      "Gradient Descent(394/999): loss=4.824161375108761, w0=-1.9464511342735138, w1=0.9972023273750388\n",
      "Gradient Descent(395/999): loss=4.822910816767899, w0=-1.9499649879244632, w1=0.9975553320884093\n",
      "Gradient Descent(396/999): loss=4.821664762866011, w0=-1.9534725075175046, w1=0.9979077004769586\n",
      "Gradient Descent(397/999): loss=4.8204231971783695, w0=-1.9569737044703845, w1=0.9982594336877233\n",
      "Gradient Descent(398/999): loss=4.819186103538688, w0=-1.9604685901802676, w1=0.998610532865672\n",
      "Gradient Descent(399/999): loss=4.817953465838902, w0=-1.9639571760237742, w1=0.9989609991537096\n",
      "Gradient Descent(400/999): loss=4.816725268028978, w0=-1.9674394733570169, w1=0.9993108336926809\n",
      "Gradient Descent(401/999): loss=4.8155014941166865, w0=-1.970915493515638, w1=0.9996600376213741\n",
      "Gradient Descent(402/999): loss=4.814282128167403, w0=-1.9743852478148467, w1=1.0000086120765248\n",
      "Gradient Descent(403/999): loss=4.813067154303901, w0=-1.9778487475494546, w1=1.000356558192819\n",
      "Gradient Descent(404/999): loss=4.811856556706141, w0=-1.9813060039939139, w1=1.0007038771028982\n",
      "Gradient Descent(405/999): loss=4.810650319611066, w0=-1.984757028402353, w1=1.0010505699373613\n",
      "Gradient Descent(406/999): loss=4.809448427312395, w0=-1.9882018320086143, w1=1.0013966378247696\n",
      "Gradient Descent(407/999): loss=4.808250864160425, w0=-1.9916404260262899, w1=1.0017420818916503\n",
      "Gradient Descent(408/999): loss=4.807057614561817, w0=-1.9950728216487579, w1=1.0020869032624995\n",
      "Gradient Descent(409/999): loss=4.805868662979403, w0=-1.9984990300492198, w1=1.0024311030597866\n",
      "Gradient Descent(410/999): loss=4.804683993931975, w0=-2.001919062380736, w1=1.0027746824039572\n",
      "Gradient Descent(411/999): loss=4.80350359199409, w0=-2.0053329297762628, w1=1.0031176424134378\n",
      "Gradient Descent(412/999): loss=4.802327441795866, w0=-2.008740643348688, w1=1.0034599842046386\n",
      "Gradient Descent(413/999): loss=4.80115552802278, w0=-2.0121422141908676, w1=1.0038017088919569\n",
      "Gradient Descent(414/999): loss=4.799987835415476, w0=-2.015537653375661, w1=1.004142817587782\n",
      "Gradient Descent(415/999): loss=4.798824348769555, w0=-2.018926971955968, w1=1.0044833114024971\n",
      "Gradient Descent(416/999): loss=4.7976650529353915, w0=-2.0223101809647654, w1=1.004823191444485\n",
      "Gradient Descent(417/999): loss=4.796509932817919, w0=-2.025687291415141, w1=1.0051624588201293\n",
      "Gradient Descent(418/999): loss=4.795358973376449, w0=-2.0290583143003307, w1=1.00550111463382\n",
      "Gradient Descent(419/999): loss=4.7942121596244665, w0=-2.0324232605937538, w1=1.005839159987956\n",
      "Gradient Descent(420/999): loss=4.793069476629436, w0=-2.0357821412490495, w1=1.0061765959829492\n",
      "Gradient Descent(421/999): loss=4.791930909512612, w0=-2.039134967200112, w1=1.006513423717228\n",
      "Gradient Descent(422/999): loss=4.790796443448837, w0=-2.042481749361125, w1=1.0068496442872403\n",
      "Gradient Descent(423/999): loss=4.789666063666355, w0=-2.0458224986266003, w1=1.0071852587874581\n",
      "Gradient Descent(424/999): loss=4.788539755446615, w0=-2.0491572258714092, w1=1.0075202683103803\n",
      "Gradient Descent(425/999): loss=4.787417504124084, w0=-2.0524859419508217, w1=1.0078546739465364\n",
      "Gradient Descent(426/999): loss=4.786299295086054, w0=-2.055808657700539, w1=1.0081884767844902\n",
      "Gradient Descent(427/999): loss=4.785185113772448, w0=-2.0591253839367307, w1=1.008521677910843\n",
      "Gradient Descent(428/999): loss=4.784074945675635, w0=-2.0624361314560686, w1=1.008854278410238\n",
      "Gradient Descent(429/999): loss=4.7829687763402395, w0=-2.0657409110357627, w1=1.0091862793653623\n",
      "Gradient Descent(430/999): loss=4.781866591362954, w0=-2.069039733433596, w1=1.0095176818569525\n",
      "Gradient Descent(431/999): loss=4.780768376392351, w0=-2.07233260938796, w1=1.0098484869637963\n",
      "Gradient Descent(432/999): loss=4.779674117128695, w0=-2.075619549617888, w1=1.0101786957627366\n",
      "Gradient Descent(433/999): loss=4.778583799323759, w0=-2.078900564823093, w1=1.0105083093286757\n",
      "Gradient Descent(434/999): loss=4.777497408780635, w0=-2.082175665684, w1=1.0108373287345784\n",
      "Gradient Descent(435/999): loss=4.776414931353552, w0=-2.08544486286178, w1=1.0111657550514748\n",
      "Gradient Descent(436/999): loss=4.775336352947692, w0=-2.0887081669983885, w1=1.0114935893484647\n",
      "Gradient Descent(437/999): loss=4.774261659519006, w0=-2.091965588716597, w1=1.011820832692721\n",
      "Gradient Descent(438/999): loss=4.773190837074032, w0=-2.095217138620028, w1=1.0121474861494921\n",
      "Gradient Descent(439/999): loss=4.772123871669708, w0=-2.09846282729319, w1=1.0124735507821072\n",
      "Gradient Descent(440/999): loss=4.771060749413197, w0=-2.1017026653015125, w1=1.012799027651978\n",
      "Gradient Descent(441/999): loss=4.770001456461701, w0=-2.1049366631913795, w1=1.0131239178186033\n",
      "Gradient Descent(442/999): loss=4.768945979022287, w0=-2.108164831490164, w1=1.0134482223395718\n",
      "Gradient Descent(443/999): loss=4.767894303351698, w0=-2.111387180706263, w1=1.013771942270566\n",
      "Gradient Descent(444/999): loss=4.7668464157561825, w0=-2.11460372132913, w1=1.014095078665365\n",
      "Gradient Descent(445/999): loss=4.765802302591315, w0=-2.117814463829311, w1=1.0144176325758494\n",
      "Gradient Descent(446/999): loss=4.764761950261812, w0=-2.121019418658478, w1=1.0147396050520026\n",
      "Gradient Descent(447/999): loss=4.763725345221363, w0=-2.1242185962494626, w1=1.0150609971419156\n",
      "Gradient Descent(448/999): loss=4.762692473972447, w0=-2.1274120070162903, w1=1.0153818098917904\n",
      "Gradient Descent(449/999): loss=4.761663323066164, w0=-2.1305996613542137, w1=1.0157020443459428\n",
      "Gradient Descent(450/999): loss=4.760637879102053, w0=-2.1337815696397477, w1=1.0160217015468063\n",
      "Gradient Descent(451/999): loss=4.7596161287279255, w0=-2.1369577422307025, w1=1.0163407825349353\n",
      "Gradient Descent(452/999): loss=4.75859805863968, w0=-2.140128189466217, w1=1.0166592883490084\n",
      "Gradient Descent(453/999): loss=4.757583655581141, w0=-2.1432929216667933, w1=1.016977220025832\n",
      "Gradient Descent(454/999): loss=4.7565729063438775, w0=-2.1464519491343292, w1=1.0172945786003436\n",
      "Gradient Descent(455/999): loss=4.755565797767038, w0=-2.149605282152153, w1=1.0176113651056147\n",
      "Gradient Descent(456/999): loss=4.7545623167371724, w0=-2.1527529309850553, w1=1.0179275805728554\n",
      "Gradient Descent(457/999): loss=4.753562450188067, w0=-2.155894905879325, w1=1.018243226031416\n",
      "Gradient Descent(458/999): loss=4.752566185100569, w0=-2.159031217062779, w1=1.0185583025087923\n",
      "Gradient Descent(459/999): loss=4.751573508502425, w0=-2.1621618747448, w1=1.0188728110306269\n",
      "Gradient Descent(460/999): loss=4.750584407468098, w0=-2.165286889116365, w1=1.0191867526207143\n",
      "Gradient Descent(461/999): loss=4.7495988691186195, w0=-2.1684062703500824, w1=1.0195001283010032\n",
      "Gradient Descent(462/999): loss=4.7486168806214, w0=-2.1715200286002228, w1=1.0198129390916004\n",
      "Gradient Descent(463/999): loss=4.7476384291900775, w0=-2.174628174002753, w1=1.0201251860107736\n",
      "Gradient Descent(464/999): loss=4.7466635020843455, w0=-2.1777307166753683, w1=1.0204368700749549\n",
      "Gradient Descent(465/999): loss=4.745692086609787, w0=-2.180827666717527, w1=1.0207479922987446\n",
      "Gradient Descent(466/999): loss=4.744724170117706, w0=-2.1839190342104806, w1=1.021058553694914\n",
      "Gradient Descent(467/999): loss=4.743759740004973, w0=-2.1870048292173094, w1=1.0213685552744083\n",
      "Gradient Descent(468/999): loss=4.742798783713851, w0=-2.1900850617829537, w1=1.0216779980463508\n",
      "Gradient Descent(469/999): loss=4.741841288731832, w0=-2.1931597419342466, w1=1.0219868830180456\n",
      "Gradient Descent(470/999): loss=4.740887242591484, w0=-2.1962288796799467, w1=1.0222952111949812\n",
      "Gradient Descent(471/999): loss=4.739936632870274, w0=-2.1992924850107713, w1=1.0226029835808335\n",
      "Gradient Descent(472/999): loss=4.738989447190423, w0=-2.2023505678994284, w1=1.0229102011774691\n",
      "Gradient Descent(473/999): loss=4.738045673218728, w0=-2.205403138300649, w1=1.023216864984949\n",
      "Gradient Descent(474/999): loss=4.737105298666416, w0=-2.2084502061512206, w1=1.0235229760015305\n",
      "Gradient Descent(475/999): loss=4.736168311288972, w0=-2.2114917813700172, w1=1.0238285352236731\n",
      "Gradient Descent(476/999): loss=4.735234698885989, w0=-2.2145278738580343, w1=1.0241335436460386\n",
      "Gradient Descent(477/999): loss=4.734304449301005, w0=-2.2175584934984194, w1=1.0244380022614963\n",
      "Gradient Descent(478/999): loss=4.733377550421342, w0=-2.220583650156505, w1=1.024741912061126\n",
      "Gradient Descent(479/999): loss=4.732453990177952, w0=-2.22360335367984, w1=1.025045274034221\n",
      "Gradient Descent(480/999): loss=4.731533756545261, w0=-2.226617613898222, w1=1.0253480891682911\n",
      "Gradient Descent(481/999): loss=4.730616837541011, w0=-2.22962644062373, w1=1.0256503584490657\n",
      "Gradient Descent(482/999): loss=4.7297032212260985, w0=-2.2326298436507557, w1=1.025952082860498\n",
      "Gradient Descent(483/999): loss=4.7287928957044265, w0=-2.2356278327560353, w1=1.026253263384767\n",
      "Gradient Descent(484/999): loss=4.7278858491227505, w0=-2.238620417698681, w1=1.0265539010022813\n",
      "Gradient Descent(485/999): loss=4.726982069670518, w0=-2.2416076082202143, w1=1.0268539966916828\n",
      "Gradient Descent(486/999): loss=4.726081545579717, w0=-2.2445894140445963, w1=1.0271535514298484\n",
      "Gradient Descent(487/999): loss=4.725184265124721, w0=-2.247565844878259, w1=1.027452566191895\n",
      "Gradient Descent(488/999): loss=4.724290216622143, w0=-2.2505369104101387, w1=1.0277510419511808\n",
      "Gradient Descent(489/999): loss=4.7233993884306775, w0=-2.2535026203117057, w1=1.0280489796793102\n",
      "Gradient Descent(490/999): loss=4.722511768950947, w0=-2.256462984236997, w1=1.0283463803461357\n",
      "Gradient Descent(491/999): loss=4.721627346625359, w0=-2.2594180118226475, w1=1.0286432449197622\n",
      "Gradient Descent(492/999): loss=4.7207461099379495, w0=-2.26236771268792, w1=1.0289395743665486\n",
      "Gradient Descent(493/999): loss=4.7198680474142325, w0=-2.2653120964347386, w1=1.0292353696511127\n",
      "Gradient Descent(494/999): loss=4.718993147621053, w0=-2.268251172647719, w1=1.0295306317363329\n",
      "Gradient Descent(495/999): loss=4.7181213991664395, w0=-2.2711849508941993, w1=1.029825361583352\n",
      "Gradient Descent(496/999): loss=4.717252790699451, w0=-2.2741134407242716, w1=1.0301195601515802\n",
      "Gradient Descent(497/999): loss=4.716387310910036, w0=-2.2770366516708136, w1=1.0304132283986989\n",
      "Gradient Descent(498/999): loss=4.715524948528875, w0=-2.2799545932495184, w1=1.0307063672806624\n",
      "Gradient Descent(499/999): loss=4.7146656923272445, w0=-2.2828672749589267, w1=1.0309989777517024\n",
      "Gradient Descent(500/999): loss=4.713809531116866, w0=-2.2857747062804568, w1=1.03129106076433\n",
      "Gradient Descent(501/999): loss=4.712956453749759, w0=-2.288676896678436, w1=1.0315826172693394\n",
      "Gradient Descent(502/999): loss=4.712106449118099, w0=-2.2915738556001313, w1=1.0318736482158113\n",
      "Gradient Descent(503/999): loss=4.711259506154067, w0=-2.2944655924757797, w1=1.032164154551115\n",
      "Gradient Descent(504/999): loss=4.710415613829716, w0=-2.2973521167186197, w1=1.0324541372209128\n",
      "Gradient Descent(505/999): loss=4.709574761156817, w0=-2.3002334377249216, w1=1.0327435971691619\n",
      "Gradient Descent(506/999): loss=4.708736937186721, w0=-2.303109564874018, w1=1.0330325353381176\n",
      "Gradient Descent(507/999): loss=4.707902131010216, w0=-2.3059805075283335, w1=1.0333209526683376\n",
      "Gradient Descent(508/999): loss=4.707070331757381, w0=-2.3088462750334173, w1=1.033608850098683\n",
      "Gradient Descent(509/999): loss=4.706241528597455, w0=-2.3117068767179716, w1=1.0338962285663238\n",
      "Gradient Descent(510/999): loss=4.70541571073868, w0=-2.3145623218938827, w1=1.0341830890067396\n",
      "Gradient Descent(511/999): loss=4.7045928674281745, w0=-2.3174126198562517, w1=1.0344694323537242\n",
      "Gradient Descent(512/999): loss=4.7037729879517896, w0=-2.3202577798834243, w1=1.034755259539388\n",
      "Gradient Descent(513/999): loss=4.702956061633963, w0=-2.3230978112370213, w1=1.0350405714941615\n",
      "Gradient Descent(514/999): loss=4.70214207783759, w0=-2.325932723161968, w1=1.0353253691467976\n",
      "Gradient Descent(515/999): loss=4.701331025963877, w0=-2.3287625248865247, w1=1.0356096534243748\n",
      "Gradient Descent(516/999): loss=4.700522895452207, w0=-2.3315872256223176, w1=1.0358934252523015\n",
      "Gradient Descent(517/999): loss=4.699717675780005, w0=-2.334406834564368, w1=1.0361766855543169\n",
      "Gradient Descent(518/999): loss=4.698915356462592, w0=-2.3372213608911214, w1=1.0364594352524954\n",
      "Gradient Descent(519/999): loss=4.6981159270530615, w0=-2.3400308137644794, w1=1.0367416752672496\n",
      "Gradient Descent(520/999): loss=4.6973193771421275, w0=-2.3428352023298276, w1=1.0370234065173325\n",
      "Gradient Descent(521/999): loss=4.696525696358007, w0=-2.3456345357160666, w1=1.0373046299198414\n",
      "Gradient Descent(522/999): loss=4.695734874366266, w0=-2.348428823035641, w1=1.03758534639022\n",
      "Gradient Descent(523/999): loss=4.694946900869703, w0=-2.35121807338457, w1=1.0378655568422621\n",
      "Gradient Descent(524/999): loss=4.694161765608201, w0=-2.354002295842475, w1=1.0381452621881142\n",
      "Gradient Descent(525/999): loss=4.6933794583586, w0=-2.356781499472612, w1=1.0384244633382789\n",
      "Gradient Descent(526/999): loss=4.692599968934568, w0=-2.359555693321899, w1=1.0387031612016169\n",
      "Gradient Descent(527/999): loss=4.691823287186457, w0=-2.3623248864209456, w1=1.038981356685351\n",
      "Gradient Descent(528/999): loss=4.691049403001182, w0=-2.3650890877840833, w1=1.0392590506950687\n",
      "Gradient Descent(529/999): loss=4.69027830630208, w0=-2.3678483064093947, w1=1.039536244134725\n",
      "Gradient Descent(530/999): loss=4.689509987048791, w0=-2.370602551278742, w1=1.039812937906645\n",
      "Gradient Descent(531/999): loss=4.688744435237113, w0=-2.3733518313577973, w1=1.040089132911528\n",
      "Gradient Descent(532/999): loss=4.687981640898881, w0=-2.3760961555960702, w1=1.0403648300484494\n",
      "Gradient Descent(533/999): loss=4.6872215941018345, w0=-2.378835532926939, w1=1.0406400302148637\n",
      "Gradient Descent(534/999): loss=4.686464284949491, w0=-2.381569972267678, w1=1.0409147343066083\n",
      "Gradient Descent(535/999): loss=4.685709703581008, w0=-2.3842994825194883, w1=1.041188943217905\n",
      "Gradient Descent(536/999): loss=4.684957840171069, w0=-2.3870240725675242, w1=1.0414626578413637\n",
      "Gradient Descent(537/999): loss=4.684208684929745, w0=-2.3897437512809248, w1=1.041735879067986\n",
      "Gradient Descent(538/999): loss=4.683462228102367, w0=-2.392458527512841, w1=1.042008607787167\n",
      "Gradient Descent(539/999): loss=4.682718459969402, w0=-2.395168410100466, w1=1.0422808448866987\n",
      "Gradient Descent(540/999): loss=4.681977370846331, w0=-2.3978734078650623, w1=1.042552591252772\n",
      "Gradient Descent(541/999): loss=4.681238951083514, w0=-2.4005735296119917, w1=1.0428238477699816\n",
      "Gradient Descent(542/999): loss=4.680503191066067, w0=-2.4032687841307427, w1=1.0430946153213267\n",
      "Gradient Descent(543/999): loss=4.679770081213745, w0=-2.405959180194961, w1=1.0433648947882153\n",
      "Gradient Descent(544/999): loss=4.679039611980803, w0=-2.408644726562476, w1=1.043634687050466\n",
      "Gradient Descent(545/999): loss=4.678311773855883, w0=-2.4113254319753312, w1=1.0439039929863123\n",
      "Gradient Descent(546/999): loss=4.677586557361888, w0=-2.414001305159811, w1=1.0441728134724042\n",
      "Gradient Descent(547/999): loss=4.676863953055852, w0=-2.41667235482647, w1=1.044441149383811\n",
      "Gradient Descent(548/999): loss=4.676143951528825, w0=-2.419338589670162, w1=1.0447090015940252\n",
      "Gradient Descent(549/999): loss=4.675426543405748, w0=-2.4220000183700656, w1=1.0449763709749644\n",
      "Gradient Descent(550/999): loss=4.674711719345328, w0=-2.4246566495897164, w1=1.045243258396975\n",
      "Gradient Descent(551/999): loss=4.6739994700399246, w0=-2.4273084919770316, w1=1.045509664728834\n",
      "Gradient Descent(552/999): loss=4.673289786215413, w0=-2.4299555541643407, w1=1.0457755908377522\n",
      "Gradient Descent(553/999): loss=4.672582658631084, w0=-2.4325978447684125, w1=1.0460410375893778\n",
      "Gradient Descent(554/999): loss=4.671878078079505, w0=-2.4352353723904825, w1=1.0463060058477982\n",
      "Gradient Descent(555/999): loss=4.671176035386413, w0=-2.4378681456162825, w1=1.046570496475543\n",
      "Gradient Descent(556/999): loss=4.6704765214105874, w0=-2.4404961730160672, w1=1.0468345103335874\n",
      "Gradient Descent(557/999): loss=4.669779527043733, w0=-2.443119463144643, w1=1.0470980482813548\n",
      "Gradient Descent(558/999): loss=4.669085043210365, w0=-2.4457380245413947, w1=1.0473611111767187\n",
      "Gradient Descent(559/999): loss=4.668393060867689, w0=-2.4483518657303147, w1=1.0476236998760067\n",
      "Gradient Descent(560/999): loss=4.667703571005477, w0=-2.45096099522003, w1=1.0478858152340025\n",
      "Gradient Descent(561/999): loss=4.667016564645957, w0=-2.4535654215038303, w1=1.0481474581039494\n",
      "Gradient Descent(562/999): loss=4.666332032843698, w0=-2.456165153059694, w1=1.0484086293375523\n",
      "Gradient Descent(563/999): loss=4.665649966685489, w0=-2.458760198350319, w1=1.0486693297849807\n",
      "Gradient Descent(564/999): loss=4.664970357290219, w0=-2.461350565823147, w1=1.048929560294872\n",
      "Gradient Descent(565/999): loss=4.664293195808771, w0=-2.4639362639103926, w1=1.0491893217143338\n",
      "Gradient Descent(566/999): loss=4.6636184734239015, w0=-2.466517301029071, w1=1.0494486148889461\n",
      "Gradient Descent(567/999): loss=4.662946181350124, w0=-2.469093685581025, w1=1.0497074406627656\n",
      "Gradient Descent(568/999): loss=4.6622763108336, w0=-2.471665425952951, w1=1.0499657998783265\n",
      "Gradient Descent(569/999): loss=4.661608853152018, w0=-2.474232530516429, w1=1.0502236933766451\n",
      "Gradient Descent(570/999): loss=4.660943799614486, w0=-2.4767950076279486, w1=1.0504811219972212\n",
      "Gradient Descent(571/999): loss=4.660281141561418, w0=-2.4793528656289343, w1=1.0507380865780418\n",
      "Gradient Descent(572/999): loss=4.659620870364415, w0=-2.4819061128457762, w1=1.0509945879555826\n",
      "Gradient Descent(573/999): loss=4.65896297742616, w0=-2.4844547575898543, w1=1.0512506269648123\n",
      "Gradient Descent(574/999): loss=4.658307454180302, w0=-2.4869988081575665, w1=1.0515062044391936\n",
      "Gradient Descent(575/999): loss=4.657654292091346, w0=-2.489538272830356, w1=1.0517613212106878\n",
      "Gradient Descent(576/999): loss=4.657003482654543, w0=-2.4920731598747383, w1=1.0520159781097556\n",
      "Gradient Descent(577/999): loss=4.6563550173957715, w0=-2.4946034775423267, w1=1.0522701759653612\n",
      "Gradient Descent(578/999): loss=4.65570888787144, w0=-2.497129234069861, w1=1.0525239156049746\n",
      "Gradient Descent(579/999): loss=4.655065085668367, w0=-2.4996504376792332, w1=1.052777197854574\n",
      "Gradient Descent(580/999): loss=4.654423602403678, w0=-2.5021670965775145, w1=1.0530300235386487\n",
      "Gradient Descent(581/999): loss=4.653784429724688, w0=-2.504679218956982, w1=1.0532823934802016\n",
      "Gradient Descent(582/999): loss=4.6531475593088025, w0=-2.507186812995146, w1=1.0535343085007525\n",
      "Gradient Descent(583/999): loss=4.6525129828634, w0=-2.509689886854775, w1=1.05378576942034\n",
      "Gradient Descent(584/999): loss=4.651880692125735, w0=-2.512188448683924, w1=1.0540367770575247\n",
      "Gradient Descent(585/999): loss=4.651250678862817, w0=-2.514682506615961, w1=1.0542873322293915\n",
      "Gradient Descent(586/999): loss=4.650622934871314, w0=-2.5171720687695913, w1=1.0545374357515525\n",
      "Gradient Descent(587/999): loss=4.649997451977443, w0=-2.5196571432488866, w1=1.0547870884381496\n",
      "Gradient Descent(588/999): loss=4.649374222036858, w0=-2.52213773814331, w1=1.055036291101857\n",
      "Gradient Descent(589/999): loss=4.648753236934556, w0=-2.524613861527742, w1=1.055285044553884\n",
      "Gradient Descent(590/999): loss=4.648134488584754, w0=-2.5270855214625083, w1=1.055533349603978\n",
      "Gradient Descent(591/999): loss=4.647517968930801, w0=-2.5295527259934047, w1=1.0557812070604262\n",
      "Gradient Descent(592/999): loss=4.646903669945062, w0=-2.5320154831517234, w1=1.0560286177300593\n",
      "Gradient Descent(593/999): loss=4.64629158362882, w0=-2.5344738009542795, w1=1.056275582418253\n",
      "Gradient Descent(594/999): loss=4.645681702012166, w0=-2.5369276874034377, w1=1.0565221019289317\n",
      "Gradient Descent(595/999): loss=4.6450740171539, w0=-2.5393771504871365, w1=1.0567681770645707\n",
      "Gradient Descent(596/999): loss=4.644468521141423, w0=-2.541822198178916, w1=1.0570138086261986\n",
      "Gradient Descent(597/999): loss=4.6438652060906405, w0=-2.5442628384379438, w1=1.0572589974134\n",
      "Gradient Descent(598/999): loss=4.643264064145854, w0=-2.546699079209039, w1=1.0575037442243178\n",
      "Gradient Descent(599/999): loss=4.642665087479663, w0=-2.549130928422701, w1=1.057748049855657\n",
      "Gradient Descent(600/999): loss=4.6420682682928565, w0=-2.5515583939951316, w1=1.0579919151026862\n",
      "Gradient Descent(601/999): loss=4.6414735988143185, w0=-2.5539814838282653, w1=1.05823534075924\n",
      "Gradient Descent(602/999): loss=4.6408810713009245, w0=-2.556400205809791, w1=1.0584783276177225\n",
      "Gradient Descent(603/999): loss=4.64029067803744, w0=-2.55881456781318, w1=1.0587208764691094\n",
      "Gradient Descent(604/999): loss=4.63970241133642, w0=-2.5612245776977107, w1=1.0589629881029503\n",
      "Gradient Descent(605/999): loss=4.6391162635381065, w0=-2.563630243308494, w1=1.0592046633073717\n",
      "Gradient Descent(606/999): loss=4.638532227010339, w0=-2.5660315724765, w1=1.0594459028690795\n",
      "Gradient Descent(607/999): loss=4.637950294148439, w0=-2.5684285730185823, w1=1.0596867075733616\n",
      "Gradient Descent(608/999): loss=4.637370457375125, w0=-2.570821252737504, w1=1.0599270782040897\n",
      "Gradient Descent(609/999): loss=4.636792709140407, w0=-2.573209619421962, w1=1.0601670155437235\n",
      "Gradient Descent(610/999): loss=4.636217041921488, w0=-2.575593680846615, w1=1.0604065203733115\n",
      "Gradient Descent(611/999): loss=4.6356434482226705, w0=-2.5779734447721063, w1=1.0606455934724945\n",
      "Gradient Descent(612/999): loss=4.635071920575256, w0=-2.58034891894509, w1=1.0608842356195078\n",
      "Gradient Descent(613/999): loss=4.634502451537443, w0=-2.582720111098256, w1=1.0611224475911842\n",
      "Gradient Descent(614/999): loss=4.633935033694242, w0=-2.585087028950355, w1=1.0613602301629559\n",
      "Gradient Descent(615/999): loss=4.633369659657368, w0=-2.5874496802062246, w1=1.0615975841088572\n",
      "Gradient Descent(616/999): loss=4.632806322065146, w0=-2.589808072556813, w1=1.0618345102015276\n",
      "Gradient Descent(617/999): loss=4.632245013582422, w0=-2.5921622136792055, w1=1.0620710092122134\n",
      "Gradient Descent(618/999): loss=4.63168572690046, w0=-2.594512111236648, w1=1.0623070819107707\n",
      "Gradient Descent(619/999): loss=4.63112845473685, w0=-2.5968577728785727, w1=1.0625427290656682\n",
      "Gradient Descent(620/999): loss=4.630573189835415, w0=-2.5991992062406233, w1=1.0627779514439888\n",
      "Gradient Descent(621/999): loss=4.630019924966111, w0=-2.6015364189446797, w1=1.0630127498114332\n",
      "Gradient Descent(622/999): loss=4.6294686529249365, w0=-2.603869418598882, w1=1.0632471249323217\n",
      "Gradient Descent(623/999): loss=4.628919366533842, w0=-2.606198212797657, w1=1.0634810775695966\n",
      "Gradient Descent(624/999): loss=4.62837205864063, w0=-2.6085228091217405, w1=1.0637146084848252\n",
      "Gradient Descent(625/999): loss=4.627826722118864, w0=-2.610843215138204, w1=1.0639477184382018\n",
      "Gradient Descent(626/999): loss=4.627283349867778, w0=-2.6131594384004786, w1=1.0641804081885504\n",
      "Gradient Descent(627/999): loss=4.626741934812184, w0=-2.6154714864483792, w1=1.0644126784933277\n",
      "Gradient Descent(628/999): loss=4.626202469902373, w0=-2.61777936680813, w1=1.064644530108624\n",
      "Gradient Descent(629/999): loss=4.625664948114038, w0=-2.6200830869923886, w1=1.0648759637891674\n",
      "Gradient Descent(630/999): loss=4.625129362448163, w0=-2.622382654500269, w1=1.0651069802883253\n",
      "Gradient Descent(631/999): loss=4.62459570593095, w0=-2.624678076817369, w1=1.0653375803581073\n",
      "Gradient Descent(632/999): loss=4.6240639716137135, w0=-2.6269693614157923, w1=1.065567764749167\n",
      "Gradient Descent(633/999): loss=4.623534152572803, w0=-2.629256515754173, w1=1.0657975342108053\n",
      "Gradient Descent(634/999): loss=4.623006241909502, w0=-2.6315395472777006, w1=1.0660268894909721\n",
      "Gradient Descent(635/999): loss=4.622480232749945, w0=-2.633818463418144, w1=1.0662558313362696\n",
      "Gradient Descent(636/999): loss=4.621956118245025, w0=-2.6360932715938756, w1=1.0664843604919532\n",
      "Gradient Descent(637/999): loss=4.621433891570306, w0=-2.6383639792098954, w1=1.0667124777019357\n",
      "Gradient Descent(638/999): loss=4.620913545925933, w0=-2.640630593657855, w1=1.0669401837087886\n",
      "Gradient Descent(639/999): loss=4.62039507453654, w0=-2.6428931223160825, w1=1.0671674792537453\n",
      "Gradient Descent(640/999): loss=4.619878470651173, w0=-2.645151572549605, w1=1.0673943650767024\n",
      "Gradient Descent(641/999): loss=4.619363727543187, w0=-2.647405951710174, w1=1.067620841916223\n",
      "Gradient Descent(642/999): loss=4.618850838510172, w0=-2.649656267136288, w1=1.0678469105095392\n",
      "Gradient Descent(643/999): loss=4.618339796873855, w0=-2.6519025261532185, w1=1.0680725715925536\n",
      "Gradient Descent(644/999): loss=4.6178305959800205, w0=-2.6541447360730315, w1=1.068297825899843\n",
      "Gradient Descent(645/999): loss=4.617323229198419, w0=-2.6563829041946128, w1=1.0685226741646592\n",
      "Gradient Descent(646/999): loss=4.616817689922686, w0=-2.6586170378036904, w1=1.0687471171189327\n",
      "Gradient Descent(647/999): loss=4.61631397157025, w0=-2.6608471441728603, w1=1.0689711554932748\n",
      "Gradient Descent(648/999): loss=4.61581206758225, w0=-2.663073230561608, w1=1.0691947900169796\n",
      "Gradient Descent(649/999): loss=4.61531197142345, w0=-2.6652953042163334, w1=1.0694180214180262\n",
      "Gradient Descent(650/999): loss=4.6148136765821555, w0=-2.667513372370374, w1=1.0696408504230823\n",
      "Gradient Descent(651/999): loss=4.614317176570121, w0=-2.669727442244029, w1=1.069863277757505\n",
      "Gradient Descent(652/999): loss=4.613822464922479, w0=-2.6719375210445815, w1=1.0700853041453442\n",
      "Gradient Descent(653/999): loss=4.613329535197642, w0=-2.6741436159663237, w1=1.0703069303093442\n",
      "Gradient Descent(654/999): loss=4.612838380977227, w0=-2.6763457341905785, w1=1.0705281569709473\n",
      "Gradient Descent(655/999): loss=4.612348995865971, w0=-2.678543882885724, w1=1.0707489848502947\n",
      "Gradient Descent(656/999): loss=4.611861373491643, w0=-2.680738069207217, w1=1.0709694146662294\n",
      "Gradient Descent(657/999): loss=4.611375507504967, w0=-2.682928300297616, w1=1.071189447136299\n",
      "Gradient Descent(658/999): loss=4.6108913915795355, w0=-2.685114583286604, w1=1.0714090829767575\n",
      "Gradient Descent(659/999): loss=4.610409019411731, w0=-2.6872969252910113, w1=1.0716283229025678\n",
      "Gradient Descent(660/999): loss=4.609928384720636, w0=-2.689475333414841, w1=1.0718471676274042\n",
      "Gradient Descent(661/999): loss=4.609449481247963, w0=-2.6916498147492893, w1=1.0720656178636538\n",
      "Gradient Descent(662/999): loss=4.6089723027579605, w0=-2.693820376372771, w1=1.0722836743224204\n",
      "Gradient Descent(663/999): loss=4.608496843037341, w0=-2.69598702535094, w1=1.0725013377135257\n",
      "Gradient Descent(664/999): loss=4.608023095895198, w0=-2.698149768736715, w1=1.0727186087455118\n",
      "Gradient Descent(665/999): loss=4.60755105516292, w0=-2.7003086135703005, w1=1.0729354881256434\n",
      "Gradient Descent(666/999): loss=4.60708071469412, w0=-2.7024635668792096, w1=1.0731519765599107\n",
      "Gradient Descent(667/999): loss=4.606612068364545, w0=-2.7046146356782894, w1=1.0733680747530312\n",
      "Gradient Descent(668/999): loss=4.606145110072004, w0=-2.7067618269697404, w1=1.0735837834084516\n",
      "Gradient Descent(669/999): loss=4.605679833736285, w0=-2.708905147743142, w1=1.0737991032283511\n",
      "Gradient Descent(670/999): loss=4.605216233299077, w0=-2.7110446049754735, w1=1.0740140349136431\n",
      "Gradient Descent(671/999): loss=4.60475430272389, w0=-2.7131802056311383, w1=1.0742285791639774\n",
      "Gradient Descent(672/999): loss=4.604294035995977, w0=-2.715311956661985, w1=1.0744427366777425\n",
      "Gradient Descent(673/999): loss=4.603835427122256, w0=-2.7174398650073317, w1=1.074656508152068\n",
      "Gradient Descent(674/999): loss=4.603378470131233, w0=-2.719563937593987, w1=1.0748698942828274\n",
      "Gradient Descent(675/999): loss=4.60292315907292, w0=-2.7216841813362733, w1=1.075082895764639\n",
      "Gradient Descent(676/999): loss=4.602469488018764, w0=-2.7238006031360498, w1=1.0752955132908697\n",
      "Gradient Descent(677/999): loss=4.602017451061564, w0=-2.7259132098827337, w1=1.075507747553636\n",
      "Gradient Descent(678/999): loss=4.6015670423153985, w0=-2.728022008453324, w1=1.0757195992438071\n",
      "Gradient Descent(679/999): loss=4.601118255915542, w0=-2.730127005712423, w1=1.0759310690510069\n",
      "Gradient Descent(680/999): loss=4.600671086018399, w0=-2.7322282085122587, w1=1.0761421576636157\n",
      "Gradient Descent(681/999): loss=4.600225526801421, w0=-2.734325623692708, w1=1.0763528657687735\n",
      "Gradient Descent(682/999): loss=4.59978157246303, w0=-2.7364192580813174, w1=1.0765631940523814\n",
      "Gradient Descent(683/999): loss=4.599339217222544, w0=-2.7385091184933263, w1=1.0767731431991043\n",
      "Gradient Descent(684/999): loss=4.598898455320108, w0=-2.74059521173169, w1=1.0769827138923727\n",
      "Gradient Descent(685/999): loss=4.5984592810166065, w0=-2.742677544587099, w1=1.0771919068143851\n",
      "Gradient Descent(686/999): loss=4.598021688593604, w0=-2.7447561238380045, w1=1.0774007226461104\n",
      "Gradient Descent(687/999): loss=4.597585672353254, w0=-2.746830956250638, w1=1.07760916206729\n",
      "Gradient Descent(688/999): loss=4.597151226618241, w0=-2.7489020485790343, w1=1.07781722575644\n",
      "Gradient Descent(689/999): loss=4.596718345731694, w0=-2.750969407565054, w1=1.0780249143908534\n",
      "Gradient Descent(690/999): loss=4.596287024057118, w0=-2.7530330399384044, w1=1.0782322286466022\n",
      "Gradient Descent(691/999): loss=4.595857255978325, w0=-2.755092952416662, w1=1.0784391691985398\n",
      "Gradient Descent(692/999): loss=4.595429035899349, w0=-2.7571491517052937, w1=1.0786457367203033\n",
      "Gradient Descent(693/999): loss=4.595002358244384, w0=-2.75920164449768, w1=1.0788519318843153\n",
      "Gradient Descent(694/999): loss=4.594577217457711, w0=-2.7612504374751357, w1=1.0790577553617862\n",
      "Gradient Descent(695/999): loss=4.594153608003618, w0=-2.7632955373069312, w1=1.079263207822717\n",
      "Gradient Descent(696/999): loss=4.59373152436633, w0=-2.7653369506503163, w1=1.0794682899358998\n",
      "Gradient Descent(697/999): loss=4.593310961049945, w0=-2.7673746841505387, w1=1.0796730023689225\n",
      "Gradient Descent(698/999): loss=4.5928919125783585, w0=-2.769408744440869, w1=1.079877345788169\n",
      "Gradient Descent(699/999): loss=4.592474373495182, w0=-2.771439138142619, w1=1.0800813208588218\n",
      "Gradient Descent(700/999): loss=4.592058338363686, w0=-2.773465871865167, w1=1.0802849282448646\n",
      "Gradient Descent(701/999): loss=4.591643801766726, w0=-2.775488952205976, w1=1.080488168609084\n",
      "Gradient Descent(702/999): loss=4.591230758306666, w0=-2.7775083857506164, w1=1.0806910426130723\n",
      "Gradient Descent(703/999): loss=4.590819202605312, w0=-2.7795241790727876, w1=1.0808935509172286\n",
      "Gradient Descent(704/999): loss=4.590409129303844, w0=-2.7815363387343397, w1=1.0810956941807621\n",
      "Gradient Descent(705/999): loss=4.590000533062744, w0=-2.783544871285294, w1=1.0812974730616935\n",
      "Gradient Descent(706/999): loss=4.589593408561723, w0=-2.7855497832638654, w1=1.0814988882168572\n",
      "Gradient Descent(707/999): loss=4.5891877504996605, w0=-2.787551081196482, w1=1.081699940301904\n",
      "Gradient Descent(708/999): loss=4.588783553594528, w0=-2.7895487715978082, w1=1.0819006299713023\n",
      "Gradient Descent(709/999): loss=4.5883808125833205, w0=-2.7915428609707647, w1=1.0821009578783414\n",
      "Gradient Descent(710/999): loss=4.587979522221995, w0=-2.79353335580655, w1=1.0823009246751323\n",
      "Gradient Descent(711/999): loss=4.587579677285392, w0=-2.7955202625846622, w1=1.0825005310126108\n",
      "Gradient Descent(712/999): loss=4.587181272567177, w0=-2.7975035877729186, w1=1.0826997775405394\n",
      "Gradient Descent(713/999): loss=4.586784302879765, w0=-2.7994833378274784, w1=1.0828986649075094\n",
      "Gradient Descent(714/999): loss=4.586388763054259, w0=-2.801459519192863, w1=1.0830971937609428\n",
      "Gradient Descent(715/999): loss=4.585994647940378, w0=-2.8034321383019756, w1=1.0832953647470946\n",
      "Gradient Descent(716/999): loss=4.585601952406392, w0=-2.8054012015761254, w1=1.0834931785110549\n",
      "Gradient Descent(717/999): loss=4.585210671339057, w0=-2.8073667154250455, w1=1.0836906356967508\n",
      "Gradient Descent(718/999): loss=4.5848207996435475, w0=-2.8093286862469147, w1=1.083887736946949\n",
      "Gradient Descent(719/999): loss=4.584432332243384, w0=-2.811287120428379, w1=1.0840844829032572\n",
      "Gradient Descent(720/999): loss=4.584045264080381, w0=-2.813242024344571, w1=1.0842808742061265\n",
      "Gradient Descent(721/999): loss=4.5836595901145625, w0=-2.815193404359133, w1=1.0844769114948543\n",
      "Gradient Descent(722/999): loss=4.583275305324111, w0=-2.817141266824235, w1=1.0846725954075842\n",
      "Gradient Descent(723/999): loss=4.582892404705298, w0=-2.8190856180805968, w1=1.084867926581311\n",
      "Gradient Descent(724/999): loss=4.582510883272419, w0=-2.8210264644575087, w1=1.0850629056518801\n",
      "Gradient Descent(725/999): loss=4.582130736057722, w0=-2.8229638122728518, w1=1.0852575332539915\n",
      "Gradient Descent(726/999): loss=4.581751958111356, w0=-2.8248976678331186, w1=1.0854518100212005\n",
      "Gradient Descent(727/999): loss=4.581374544501292, w0=-2.8268280374334336, w1=1.085645736585921\n",
      "Gradient Descent(728/999): loss=4.580998490313272, w0=-2.8287549273575734, w1=1.085839313579426\n",
      "Gradient Descent(729/999): loss=4.580623790650735, w0=-2.8306783438779877, w1=1.0860325416318515\n",
      "Gradient Descent(730/999): loss=4.580250440634761, w0=-2.8325982932558196, w1=1.0862254213721971\n",
      "Gradient Descent(731/999): loss=4.579878435403997, w0=-2.834514781740926, w1=1.0864179534283287\n",
      "Gradient Descent(732/999): loss=4.579507770114606, w0=-2.8364278155718976, w1=1.0866101384269806\n",
      "Gradient Descent(733/999): loss=4.579138439940198, w0=-2.8383374009760796, w1=1.086801976993757\n",
      "Gradient Descent(734/999): loss=4.5787704400717635, w0=-2.8402435441695912, w1=1.0869934697531343\n",
      "Gradient Descent(735/999): loss=4.578403765717619, w0=-2.8421462513573474, w1=1.087184617328464\n",
      "Gradient Descent(736/999): loss=4.578038412103337, w0=-2.844045528733078, w1=1.087375420341973\n",
      "Gradient Descent(737/999): loss=4.57767437447169, w0=-2.8459413824793476, w1=1.0875658794147673\n",
      "Gradient Descent(738/999): loss=4.577311648082583, w0=-2.8478338187675765, w1=1.0877559951668327\n",
      "Gradient Descent(739/999): loss=4.576950228212997, w0=-2.84972284375806, w1=1.087945768217038\n",
      "Gradient Descent(740/999): loss=4.576590110156924, w0=-2.8516084635999897, w1=1.088135199183136\n",
      "Gradient Descent(741/999): loss=4.576231289225308, w0=-2.8534906844314714, w1=1.0883242886817661\n",
      "Gradient Descent(742/999): loss=4.57587376074598, w0=-2.8553695123795473, w1=1.088513037328456\n",
      "Gradient Descent(743/999): loss=4.575517520063601, w0=-2.8572449535602154, w1=1.088701445737624\n",
      "Gradient Descent(744/999): loss=4.575162562539603, w0=-2.859117014078448, w1=1.0888895145225812\n",
      "Gradient Descent(745/999): loss=4.574808883552121, w0=-2.860985700028213, w1=1.0890772442955325\n",
      "Gradient Descent(746/999): loss=4.574456478495943, w0=-2.8628510174924937, w1=1.0892646356675795\n",
      "Gradient Descent(747/999): loss=4.574105342782438, w0=-2.864712972543308, w1=1.0894516892487223\n",
      "Gradient Descent(748/999): loss=4.573755471839511, w0=-2.866571571241728, w1=1.0896384056478612\n",
      "Gradient Descent(749/999): loss=4.573406861111529, w0=-2.868426819637901, w1=1.0898247854727994\n",
      "Gradient Descent(750/999): loss=4.573059506059272, w0=-2.8702787237710674, w1=1.090010829330244\n",
      "Gradient Descent(751/999): loss=4.5727134021598665, w0=-2.872127289669582, w1=1.0901965378258085\n",
      "Gradient Descent(752/999): loss=4.572368544906735, w0=-2.8739725233509326, w1=1.090381911564015\n",
      "Gradient Descent(753/999): loss=4.572024929809525, w0=-2.8758144308217597, w1=1.0905669511482956\n",
      "Gradient Descent(754/999): loss=4.571682552394068, w0=-2.877653018077877, w1=1.0907516571809948\n",
      "Gradient Descent(755/999): loss=4.5713414082023025, w0=-2.879488291104289, w1=1.0909360302633713\n",
      "Gradient Descent(756/999): loss=4.571001492792229, w0=-2.8813202558752127, w1=1.0911200709956\n",
      "Gradient Descent(757/999): loss=4.5706628017378454, w0=-2.8831489183540957, w1=1.0913037799767737\n",
      "Gradient Descent(758/999): loss=4.570325330629095, w0=-2.8849742844936355, w1=1.0914871578049057\n",
      "Gradient Descent(759/999): loss=4.569989075071803, w0=-2.8867963602357998, w1=1.091670205076931\n",
      "Gradient Descent(760/999): loss=4.569654030687624, w0=-2.8886151515118454, w1=1.0918529223887086\n",
      "Gradient Descent(761/999): loss=4.56932019311398, w0=-2.890430664242337, w1=1.0920353103350233\n",
      "Gradient Descent(762/999): loss=4.568987558004012, w0=-2.892242904337167, w1=1.0922173695095885\n",
      "Gradient Descent(763/999): loss=4.568656121026514, w0=-2.8940518776955746, w1=1.0923991005050462\n",
      "Gradient Descent(764/999): loss=4.568325877865882, w0=-2.8958575902061656, w1=1.0925805039129708\n",
      "Gradient Descent(765/999): loss=4.567996824222056, w0=-2.8976600477469305, w1=1.0927615803238704\n",
      "Gradient Descent(766/999): loss=4.567668955810467, w0=-2.8994592561852643, w1=1.0929423303271883\n",
      "Gradient Descent(767/999): loss=4.567342268361972, w0=-2.9012552213779856, w1=1.0931227545113058\n",
      "Gradient Descent(768/999): loss=4.567016757622818, w0=-2.9030479491713552, w1=1.0933028534635427\n",
      "Gradient Descent(769/999): loss=4.56669241935456, w0=-2.904837445401096, w1=1.093482627770161\n",
      "Gradient Descent(770/999): loss=4.566369249334026, w0=-2.906623715892411, w1=1.0936620780163653\n",
      "Gradient Descent(771/999): loss=4.566047243353257, w0=-2.9084067664600024, w1=1.0938412047863058\n",
      "Gradient Descent(772/999): loss=4.565726397219448, w0=-2.9101866029080914, w1=1.0940200086630791\n",
      "Gradient Descent(773/999): loss=4.565406706754898, w0=-2.9119632310304366, w1=1.0941984902287314\n",
      "Gradient Descent(774/999): loss=4.565088167796951, w0=-2.9137366566103524, w1=1.0943766500642593\n",
      "Gradient Descent(775/999): loss=4.564770776197949, w0=-2.9155068854207284, w1=1.0945544887496121\n",
      "Gradient Descent(776/999): loss=4.564454527825168, w0=-2.917273923224048, w1=1.094732006863694\n",
      "Gradient Descent(777/999): loss=4.564139418560777, w0=-2.9190377757724075, w1=1.0949092049843652\n",
      "Gradient Descent(778/999): loss=4.563825444301772, w0=-2.920798448807534, w1=1.0950860836884444\n",
      "Gradient Descent(779/999): loss=4.563512600959929, w0=-2.922555948060804, w1=1.0952626435517108\n",
      "Gradient Descent(780/999): loss=4.5632008844617475, w0=-2.9243102792532647, w1=1.0954388851489054\n",
      "Gradient Descent(781/999): loss=4.562890290748406, w0=-2.9260614480956484, w1=1.0956148090537334\n",
      "Gradient Descent(782/999): loss=4.562580815775695, w0=-2.9278094602883944, w1=1.0957904158388658\n",
      "Gradient Descent(783/999): loss=4.562272455513976, w0=-2.929554321521666, w1=1.0959657060759411\n",
      "Gradient Descent(784/999): loss=4.561965205948124, w0=-2.93129603747537, w1=1.0961406803355676\n",
      "Gradient Descent(785/999): loss=4.561659063077478, w0=-2.933034613819174, w1=1.0963153391873246\n",
      "Gradient Descent(786/999): loss=4.561354022915784, w0=-2.9347700562125256, w1=1.0964896831997653\n",
      "Gradient Descent(787/999): loss=4.56105008149115, w0=-2.936502370304671, w1=1.0966637129404178\n",
      "Gradient Descent(788/999): loss=4.560747234845985, w0=-2.9382315617346726, w1=1.096837428975787\n",
      "Gradient Descent(789/999): loss=4.560445479036962, w0=-2.939957636131428, w1=1.0970108318713567\n",
      "Gradient Descent(790/999): loss=4.560144810134948, w0=-2.9416805991136887, w1=1.0971839221915918\n",
      "Gradient Descent(791/999): loss=4.559845224224967, w0=-2.9434004562900773, w1=1.0973567004999385\n",
      "Gradient Descent(792/999): loss=4.559546717406144, w0=-2.9451172132591066, w1=1.0975291673588288\n",
      "Gradient Descent(793/999): loss=4.559249285791658, w0=-2.9468308756091974, w1=1.0977013233296797\n",
      "Gradient Descent(794/999): loss=4.558952925508681, w0=-2.948541448918697, w1=1.097873168972897\n",
      "Gradient Descent(795/999): loss=4.558657632698341, w0=-2.9502489387558963, w1=1.0980447048478763\n",
      "Gradient Descent(796/999): loss=4.558363403515664, w0=-2.9519533506790503, w1=1.0982159315130038\n",
      "Gradient Descent(797/999): loss=4.558070234129523, w0=-2.953654690236394, w1=1.0983868495256603\n",
      "Gradient Descent(798/999): loss=4.557778120722592, w0=-2.955352962966161, w1=1.0985574594422216\n",
      "Gradient Descent(799/999): loss=4.557487059491295, w0=-2.957048174396602, w1=1.09872776181806\n",
      "Gradient Descent(800/999): loss=4.557197046645755, w0=-2.9587403300460022, w1=1.0988977572075473\n",
      "Gradient Descent(801/999): loss=4.556908078409751, w0=-2.9604294354226997, w1=1.0990674461640562\n",
      "Gradient Descent(802/999): loss=4.556620151020654, w0=-2.9621154960251035, w1=1.0992368292399608\n",
      "Gradient Descent(803/999): loss=4.556333260729399, w0=-2.963798517341711, w1=1.0994059069866409\n",
      "Gradient Descent(804/999): loss=4.556047403800417, w0=-2.965478504851126, w1=1.0995746799544812\n",
      "Gradient Descent(805/999): loss=4.5557625765115946, w0=-2.9671554640220763, w1=1.0997431486928748\n",
      "Gradient Descent(806/999): loss=4.555478775154231, w0=-2.968829400313433, w1=1.0999113137502248\n",
      "Gradient Descent(807/999): loss=4.555195996032977, w0=-2.9705003191742256, w1=1.100079175673945\n",
      "Gradient Descent(808/999): loss=4.554914235465802, w0=-2.972168226043662, w1=1.1002467350104628\n",
      "Gradient Descent(809/999): loss=4.554633489783926, w0=-2.973833126351145, w1=1.100413992305221\n",
      "Gradient Descent(810/999): loss=4.554353755331798, w0=-2.9754950255162913, w1=1.1005809481026785\n",
      "Gradient Descent(811/999): loss=4.554075028467021, w0=-2.977153928948947, w1=1.1007476029463132\n",
      "Gradient Descent(812/999): loss=4.553797305560327, w0=-2.978809842049207, w1=1.100913957378623\n",
      "Gradient Descent(813/999): loss=4.553520582995515, w0=-2.9804627702074318, w1=1.1010800119411286\n",
      "Gradient Descent(814/999): loss=4.553244857169412, w0=-2.9821127188042658, w1=1.1012457671743734\n",
      "Gradient Descent(815/999): loss=4.552970124491824, w0=-2.983759693210654, w1=1.1014112236179276\n",
      "Gradient Descent(816/999): loss=4.552696381385485, w0=-2.985403698787859, w1=1.1015763818103879\n",
      "Gradient Descent(817/999): loss=4.5524236242860185, w0=-2.9870447408874803, w1=1.1017412422893806\n",
      "Gradient Descent(818/999): loss=4.5521518496418825, w0=-2.9886828248514705, w1=1.101905805591563\n",
      "Gradient Descent(819/999): loss=4.551881053914331, w0=-2.9903179560121522, w1=1.1020700722526242\n",
      "Gradient Descent(820/999): loss=4.551611233577363, w0=-2.9919501396922366, w1=1.1022340428072888\n",
      "Gradient Descent(821/999): loss=4.551342385117677, w0=-2.9935793812048392, w1=1.1023977177893167\n",
      "Gradient Descent(822/999): loss=4.551074505034629, w0=-2.9952056858534997, w1=1.102561097731506\n",
      "Gradient Descent(823/999): loss=4.550807589840181, w0=-2.996829058932196, w1=1.1027241831656942\n",
      "Gradient Descent(824/999): loss=4.55054163605886, w0=-2.9984495057253646, w1=1.10288697462276\n",
      "Gradient Descent(825/999): loss=4.550276640227711, w0=-3.000067031507915, w1=1.1030494726326259\n",
      "Gradient Descent(826/999): loss=4.550012598896254, w0=-3.001681641545249, w1=1.1032116777242582\n",
      "Gradient Descent(827/999): loss=4.549749508626434, w0=-3.0032933410932765, w1=1.10337359042567\n",
      "Gradient Descent(828/999): loss=4.549487365992583, w0=-3.0049021353984338, w1=1.1035352112639227\n",
      "Gradient Descent(829/999): loss=4.549226167581372, w0=-3.006508029697699, w1=1.103696540765128\n",
      "Gradient Descent(830/999): loss=4.5489659099917645, w0=-3.008111029218611, w1=1.1038575794544485\n",
      "Gradient Descent(831/999): loss=4.548706589834975, w0=-3.0097111391792852, w1=1.1040183278561004\n",
      "Gradient Descent(832/999): loss=4.548448203734425, w0=-3.0113083647884307, w1=1.1041787864933554\n",
      "Gradient Descent(833/999): loss=4.548190748325698, w0=-3.0129027112453675, w1=1.1043389558885413\n",
      "Gradient Descent(834/999): loss=4.547934220256497, w0=-3.014494183740043, w1=1.1044988365630448\n",
      "Gradient Descent(835/999): loss=4.547678616186597, w0=-3.0160827874530503, w1=1.1046584290373125\n",
      "Gradient Descent(836/999): loss=4.547423932787805, w0=-3.0176685275556423, w1=1.104817733830853\n",
      "Gradient Descent(837/999): loss=4.547170166743919, w0=-3.019251409209752, w1=1.1049767514622382\n",
      "Gradient Descent(838/999): loss=4.546917314750677, w0=-3.020831437568006, w1=1.1051354824491055\n",
      "Gradient Descent(839/999): loss=4.546665373515724, w0=-3.022408617773744, w1=1.1052939273081595\n",
      "Gradient Descent(840/999): loss=4.54641433975856, w0=-3.023982954961034, w1=1.1054520865551725\n",
      "Gradient Descent(841/999): loss=4.546164210210503, w0=-3.0255544542546886, w1=1.1056099607049876\n",
      "Gradient Descent(842/999): loss=4.545914981614642, w0=-3.0271231207702836, w1=1.10576755027152\n",
      "Gradient Descent(843/999): loss=4.545666650725801, w0=-3.0286889596141724, w1=1.1059248557677583\n",
      "Gradient Descent(844/999): loss=4.54541921431049, w0=-3.0302519758835045, w1=1.1060818777057666\n",
      "Gradient Descent(845/999): loss=4.545172669146869, w0=-3.0318121746662405, w1=1.1062386165966855\n",
      "Gradient Descent(846/999): loss=4.544927012024698, w0=-3.0333695610411704, w1=1.1063950729507346\n",
      "Gradient Descent(847/999): loss=4.544682239745306, w0=-3.034924140077929, w1=1.1065512472772134\n",
      "Gradient Descent(848/999): loss=4.544438349121537, w0=-3.036475916837012, w1=1.106707140084504\n",
      "Gradient Descent(849/999): loss=4.5441953369777215, w0=-3.038024896369793, w1=1.106862751880072\n",
      "Gradient Descent(850/999): loss=4.543953200149624, w0=-3.0395710837185415, w1=1.1070180831704672\n",
      "Gradient Descent(851/999): loss=4.543711935484408, w0=-3.041114483916436, w1=1.1071731344613276\n",
      "Gradient Descent(852/999): loss=4.543471539840591, w0=-3.042655101987583, w1=1.1073279062573789\n",
      "Gradient Descent(853/999): loss=4.543232010088012, w0=-3.044192942947033, w1=1.1074823990624372\n",
      "Gradient Descent(854/999): loss=4.542993343107778, w0=-3.0457280118007954, w1=1.1076366133794109\n",
      "Gradient Descent(855/999): loss=4.542755535792234, w0=-3.047260313545857, w1=1.1077905497103013\n",
      "Gradient Descent(856/999): loss=4.542518585044916, w0=-3.0487898531701956, w1=1.1079442085562048\n",
      "Gradient Descent(857/999): loss=4.5422824877805175, w0=-3.050316635652799, w1=1.1080975904173147\n",
      "Gradient Descent(858/999): loss=4.5420472409248385, w0=-3.051840665963679, w1=1.1082506957929228\n",
      "Gradient Descent(859/999): loss=4.541812841414758, w0=-3.0533619490638895, w1=1.1084035251814206\n",
      "Gradient Descent(860/999): loss=4.541579286198186, w0=-3.0548804899055404, w1=1.1085560790803013\n",
      "Gradient Descent(861/999): loss=4.541346572234027, w0=-3.0563962934318156, w1=1.1087083579861612\n",
      "Gradient Descent(862/999): loss=4.541114696492135, w0=-3.057909364576988, w1=1.108860362394702\n",
      "Gradient Descent(863/999): loss=4.540883655953286, w0=-3.059419708266437, w1=1.109012092800731\n",
      "Gradient Descent(864/999): loss=4.540653447609125, w0=-3.060927329416663, w1=1.1091635496981642\n",
      "Gradient Descent(865/999): loss=4.540424068462133, w0=-3.062432232935303, w1=1.1093147335800269\n",
      "Gradient Descent(866/999): loss=4.540195515525591, w0=-3.063934423721149, w1=1.109465644938456\n",
      "Gradient Descent(867/999): loss=4.5399677858235385, w0=-3.0654339066641616, w1=1.1096162842647008\n",
      "Gradient Descent(868/999): loss=4.53974087639073, w0=-3.0669306866454873, w1=1.1097666520491256\n",
      "Gradient Descent(869/999): loss=4.539514784272604, w0=-3.068424768537473, w1=1.1099167487812107\n",
      "Gradient Descent(870/999): loss=4.539289506525238, w0=-3.0699161572036835, w1=1.1100665749495537\n",
      "Gradient Descent(871/999): loss=4.539065040215319, w0=-3.0714048574989166, w1=1.1102161310418717\n",
      "Gradient Descent(872/999): loss=4.538841382420094, w0=-3.072890874269218, w1=1.1103654175450026\n",
      "Gradient Descent(873/999): loss=4.5386185302273425, w0=-3.074374212351899, w1=1.110514434944907\n",
      "Gradient Descent(874/999): loss=4.538396480735327, w0=-3.0758548765755505, w1=1.1106631837266687\n",
      "Gradient Descent(875/999): loss=4.538175231052767, w0=-3.07733287176006, w1=1.1108116643744983\n",
      "Gradient Descent(876/999): loss=4.537954778298796, w0=-3.0788082027166257, w1=1.1109598773717324\n",
      "Gradient Descent(877/999): loss=4.537735119602923, w0=-3.0802808742477743, w1=1.1111078232008371\n",
      "Gradient Descent(878/999): loss=4.537516252104996, w0=-3.0817508911473745, w1=1.1112555023434085\n",
      "Gradient Descent(879/999): loss=4.537298172955165, w0=-3.083218258200654, w1=1.1114029152801745\n",
      "Gradient Descent(880/999): loss=4.537080879313848, w0=-3.0846829801842155, w1=1.1115500624909966\n",
      "Gradient Descent(881/999): loss=4.5368643683516865, w0=-3.0861450618660498, w1=1.1116969444548714\n",
      "Gradient Descent(882/999): loss=4.536648637249516, w0=-3.087604508005554, w1=1.111843561649932\n",
      "Gradient Descent(883/999): loss=4.536433683198326, w0=-3.0890613233535458, w1=1.1119899145534495\n",
      "Gradient Descent(884/999): loss=4.536219503399223, w0=-3.0905155126522788, w1=1.1121360036418346\n",
      "Gradient Descent(885/999): loss=4.536006095063398, w0=-3.0919670806354587, w1=1.1122818293906396\n",
      "Gradient Descent(886/999): loss=4.535793455412081, w0=-3.0934160320282578, w1=1.112427392274559\n",
      "Gradient Descent(887/999): loss=4.535581581676518, w0=-3.0948623715473307, w1=1.112572692767432\n",
      "Gradient Descent(888/999): loss=4.535370471097922, w0=-3.0963061039008304, w1=1.1127177313422438\n",
      "Gradient Descent(889/999): loss=4.53516012092745, w0=-3.0977472337884224, w1=1.1128625084711268\n",
      "Gradient Descent(890/999): loss=4.53495052842615, w0=-3.0991857659013013, w1=1.1130070246253623\n",
      "Gradient Descent(891/999): loss=4.534741690864944, w0=-3.100621704922205, w1=1.1131512802753818\n",
      "Gradient Descent(892/999): loss=4.534533605524584, w0=-3.1020550555254296, w1=1.1132952758907695\n",
      "Gradient Descent(893/999): loss=4.534326269695611, w0=-3.1034858223768462, w1=1.1134390119402626\n",
      "Gradient Descent(894/999): loss=4.5341196806783275, w0=-3.1049140101339154, w1=1.1135824888917536\n",
      "Gradient Descent(895/999): loss=4.533913835782764, w0=-3.1063396234457015, w1=1.1137257072122915\n",
      "Gradient Descent(896/999): loss=4.533708732328634, w0=-3.107762666952889, w1=1.113868667368083\n",
      "Gradient Descent(897/999): loss=4.5335043676453095, w0=-3.109183145287797, w1=1.1140113698244951\n",
      "Gradient Descent(898/999): loss=4.533300739071782, w0=-3.1106010630743945, w1=1.1141538150460557\n",
      "Gradient Descent(899/999): loss=4.533097843956623, w0=-3.112016424928315, w1=1.114296003496455\n",
      "Gradient Descent(900/999): loss=4.532895679657961, w0=-3.1134292354568713, w1=1.114437935638548\n",
      "Gradient Descent(901/999): loss=4.532694243543436, w0=-3.114839499259073, w1=1.1145796119343547\n",
      "Gradient Descent(902/999): loss=4.532493532990172, w0=-3.1162472209256378, w1=1.1147210328450625\n",
      "Gradient Descent(903/999): loss=4.532293545384737, w0=-3.117652405039009, w1=1.1148621988310277\n",
      "Gradient Descent(904/999): loss=4.532094278123115, w0=-3.1190550561733694, w1=1.1150031103517763\n",
      "Gradient Descent(905/999): loss=4.53189572861067, w0=-3.120455178894656, w1=1.1151437678660066\n",
      "Gradient Descent(906/999): loss=4.531697894262111, w0=-3.121852777760576, w1=1.1152841718315896\n",
      "Gradient Descent(907/999): loss=4.531500772501458, w0=-3.1232478573206204, w1=1.115424322705571\n",
      "Gradient Descent(908/999): loss=4.531304360762013, w0=-3.1246404221160793, w1=1.1155642209441723\n",
      "Gradient Descent(909/999): loss=4.531108656486318, w0=-3.1260304766800573, w1=1.1157038670027934\n",
      "Gradient Descent(910/999): loss=4.5309136571261295, w0=-3.127418025537487, w1=1.1158432613360127\n",
      "Gradient Descent(911/999): loss=4.530719360142384, w0=-3.128803073205144, w1=1.1159824043975897\n",
      "Gradient Descent(912/999): loss=4.53052576300516, w0=-3.1301856241916632, w1=1.116121296640465\n",
      "Gradient Descent(913/999): loss=4.530332863193653, w0=-3.1315656829975516, w1=1.1162599385167638\n",
      "Gradient Descent(914/999): loss=4.530140658196133, w0=-3.132943254115203, w1=1.1163983304777958\n",
      "Gradient Descent(915/999): loss=4.529949145509925, w0=-3.1343183420289145, w1=1.116536472974057\n",
      "Gradient Descent(916/999): loss=4.52975832264136, w0=-3.1356909512148987, w1=1.1166743664552319\n",
      "Gradient Descent(917/999): loss=4.529568187105754, w0=-3.1370610861413, w1=1.1168120113701938\n",
      "Gradient Descent(918/999): loss=4.529378736427375, w0=-3.1384287512682083, w1=1.116949408167007\n",
      "Gradient Descent(919/999): loss=4.529189968139407, w0=-3.1397939510476736, w1=1.1170865572929285\n",
      "Gradient Descent(920/999): loss=4.5290018797839195, w0=-3.1411566899237213, w1=1.1172234591944088\n",
      "Gradient Descent(921/999): loss=4.5288144689118335, w0=-3.1425169723323654, w1=1.1173601143170935\n",
      "Gradient Descent(922/999): loss=4.5286277330828915, w0=-3.143874802701624, w1=1.1174965231058251\n",
      "Gradient Descent(923/999): loss=4.528441669865631, w0=-3.145230185451533, w1=1.1176326860046442\n",
      "Gradient Descent(924/999): loss=4.528256276837341, w0=-3.1465831249941605, w1=1.1177686034567906\n",
      "Gradient Descent(925/999): loss=4.528071551584041, w0=-3.1479336257336223, w1=1.117904275904706\n",
      "Gradient Descent(926/999): loss=4.527887491700442, w0=-3.1492816920660944, w1=1.1180397037900336\n",
      "Gradient Descent(927/999): loss=4.527704094789922, w0=-3.1506273283798287, w1=1.118174887553621\n",
      "Gradient Descent(928/999): loss=4.527521358464489, w0=-3.151970539055167, w1=1.1183098276355212\n",
      "Gradient Descent(929/999): loss=4.527339280344758, w0=-3.1533113284645546, w1=1.1184445244749937\n",
      "Gradient Descent(930/999): loss=4.527157858059903, w0=-3.1546497009725556, w1=1.118578978510506\n",
      "Gradient Descent(931/999): loss=4.52697708924765, w0=-3.1559856609358663, w1=1.1187131901797358\n",
      "Gradient Descent(932/999): loss=4.526796971554226, w0=-3.15731921270333, w1=1.1188471599195713\n",
      "Gradient Descent(933/999): loss=4.52661750263434, w0=-3.1586503606159497, w1=1.1189808881661139\n",
      "Gradient Descent(934/999): loss=4.526438680151146, w0=-3.1599791090069047, w1=1.119114375354678\n",
      "Gradient Descent(935/999): loss=4.526260501776218, w0=-3.161305462201563, w1=1.1192476219197935\n",
      "Gradient Descent(936/999): loss=4.526082965189512, w0=-3.1626294245174944, w1=1.119380628295208\n",
      "Gradient Descent(937/999): loss=4.525906068079348, w0=-3.163951000264488, w1=1.1195133949138856\n",
      "Gradient Descent(938/999): loss=4.525729808142366, w0=-3.165270193744562, w1=1.1196459222080113\n",
      "Gradient Descent(939/999): loss=4.525554183083503, w0=-3.166587009251982, w1=1.1197782106089904\n",
      "Gradient Descent(940/999): loss=4.5253791906159675, w0=-3.167901451073271, w1=1.1199102605474507\n",
      "Gradient Descent(941/999): loss=4.525204828461203, w0=-3.169213523487225, w1=1.1200420724532438\n",
      "Gradient Descent(942/999): loss=4.525031094348856, w0=-3.1705232307649287, w1=1.1201736467554462\n",
      "Gradient Descent(943/999): loss=4.524857986016757, w0=-3.1718305771697666, w1=1.1203049838823613\n",
      "Gradient Descent(944/999): loss=4.524685501210879, w0=-3.1731355669574377, w1=1.1204360842615202\n",
      "Gradient Descent(945/999): loss=4.5245136376853194, w0=-3.174438204375971, w1=1.1205669483196836\n",
      "Gradient Descent(946/999): loss=4.52434239320226, w0=-3.175738493665737, w1=1.1206975764828424\n",
      "Gradient Descent(947/999): loss=4.524171765531945, w0=-3.1770364390594628, w1=1.1208279691762202\n",
      "Gradient Descent(948/999): loss=4.524001752452653, w0=-3.1783320447822456, w1=1.1209581268242739\n",
      "Gradient Descent(949/999): loss=4.523832351750661, w0=-3.1796253150515663, w1=1.121088049850695\n",
      "Gradient Descent(950/999): loss=4.523663561220222, w0=-3.180916254077304, w1=1.1212177386784117\n",
      "Gradient Descent(951/999): loss=4.523495378663532, w0=-3.182204866061748, w1=1.1213471937295894\n",
      "Gradient Descent(952/999): loss=4.523327801890707, w0=-3.1834911551996137, w1=1.1214764154256327\n",
      "Gradient Descent(953/999): loss=4.523160828719747, w0=-3.1847751256780543, w1=1.1216054041871868\n",
      "Gradient Descent(954/999): loss=4.5229944569765115, w0=-3.186056781676676, w1=1.1217341604341384\n",
      "Gradient Descent(955/999): loss=4.522828684494693, w0=-3.18733612736755, w1=1.1218626845856172\n",
      "Gradient Descent(956/999): loss=4.52266350911579, w0=-3.188613166915228, w1=1.1219909770599976\n",
      "Gradient Descent(957/999): loss=4.522498928689068, w0=-3.1898879044767536, w1=1.1221190382748998\n",
      "Gradient Descent(958/999): loss=4.522334941071547, w0=-3.1911603442016774, w1=1.1222468686471911\n",
      "Gradient Descent(959/999): loss=4.52217154412796, w0=-3.1924304902320704, w1=1.1223744685929875\n",
      "Gradient Descent(960/999): loss=4.522008735730734, w0=-3.1936983467025364, w1=1.1225018385276548\n",
      "Gradient Descent(961/999): loss=4.521846513759959, w0=-3.1949639177402265, w1=1.1226289788658101\n",
      "Gradient Descent(962/999): loss=4.521684876103361, w0=-3.196227207464853, w1=1.1227558900213233\n",
      "Gradient Descent(963/999): loss=4.521523820656275, w0=-3.1974882199887005, w1=1.1228825724073177\n",
      "Gradient Descent(964/999): loss=4.521363345321613, w0=-3.198746959416642, w1=1.1230090264361725\n",
      "Gradient Descent(965/999): loss=4.521203448009848, w0=-3.2000034298461504, w1=1.1231352525195233\n",
      "Gradient Descent(966/999): loss=4.52104412663897, w0=-3.201257635367313, w1=1.1232612510682638\n",
      "Gradient Descent(967/999): loss=4.520885379134478, w0=-3.202509580062844, w1=1.1233870224925466\n",
      "Gradient Descent(968/999): loss=4.520727203429336, w0=-3.2037592680080986, w1=1.1235125672017858\n",
      "Gradient Descent(969/999): loss=4.520569597463956, w0=-3.205006703271085, w1=1.1236378856046567\n",
      "Gradient Descent(970/999): loss=4.520412559186166, w0=-3.206251889912479, w1=1.123762978109098\n",
      "Gradient Descent(971/999): loss=4.52025608655119, w0=-3.2074948319856365, w1=1.1238878451223138\n",
      "Gradient Descent(972/999): loss=4.520100177521617, w0=-3.208735533536607, w1=1.1240124870507733\n",
      "Gradient Descent(973/999): loss=4.51994483006737, w0=-3.209973998604146, w1=1.1241369043002134\n",
      "Gradient Descent(974/999): loss=4.519790042165688, w0=-3.211210231219729, w1=1.1242610972756395\n",
      "Gradient Descent(975/999): loss=4.5196358118010975, w0=-3.2124442354075655, w1=1.124385066381327\n",
      "Gradient Descent(976/999): loss=4.519482136965378, w0=-3.2136760151846095, w1=1.1245088120208229\n",
      "Gradient Descent(977/999): loss=4.519329015657548, w0=-3.2149055745605746, w1=1.124632334596946\n",
      "Gradient Descent(978/999): loss=4.519176445883833, w0=-3.2161329175379465, w1=1.1247556345117895\n",
      "Gradient Descent(979/999): loss=4.519024425657637, w0=-3.217358048111996, w1=1.1248787121667216\n",
      "Gradient Descent(980/999): loss=4.518872952999525, w0=-3.2185809702707924, w1=1.1250015679623875\n",
      "Gradient Descent(981/999): loss=4.518722025937183, w0=-3.2198016879952154, w1=1.1251242022987094\n",
      "Gradient Descent(982/999): loss=4.518571642505413, w0=-3.2210202052589696, w1=1.125246615574889\n",
      "Gradient Descent(983/999): loss=4.518421800746084, w0=-3.222236526028596, w1=1.1253688081894087\n",
      "Gradient Descent(984/999): loss=4.518272498708126, w0=-3.2234506542634853, w1=1.1254907805400318\n",
      "Gradient Descent(985/999): loss=4.518123734447491, w0=-3.224662593915892, w1=1.1256125330238056\n",
      "Gradient Descent(986/999): loss=4.517975506027139, w0=-3.2258723489309458, w1=1.1257340660370607\n",
      "Gradient Descent(987/999): loss=4.5178278115170025, w0=-3.2270799232466643, w1=1.125855379975414\n",
      "Gradient Descent(988/999): loss=4.517680648993969, w0=-3.2282853207939675, w1=1.125976475233769\n",
      "Gradient Descent(989/999): loss=4.517534016541851, w0=-3.229488545496689, w1=1.1260973522063173\n",
      "Gradient Descent(990/999): loss=4.517387912251361, w0=-3.2306896012715893, w1=1.12621801128654\n",
      "Gradient Descent(991/999): loss=4.517242334220096, w0=-3.231888492028369, w1=1.126338452867209\n",
      "Gradient Descent(992/999): loss=4.517097280552497, w0=-3.23308522166968, w1=1.126458677340388\n",
      "Gradient Descent(993/999): loss=4.51695274935984, w0=-3.2342797940911403, w1=1.1265786850974338\n",
      "Gradient Descent(994/999): loss=4.516808738760196, w0=-3.2354722131813456, w1=1.1266984765289985\n",
      "Gradient Descent(995/999): loss=4.516665246878423, w0=-3.2366624828218815, w1=1.126818052025029\n",
      "Gradient Descent(996/999): loss=4.516522271846126, w0=-3.237850606887337, w1=1.1269374119747702\n",
      "Gradient Descent(997/999): loss=4.516379811801645, w0=-3.239036589245317, w1=1.1270565567667645\n",
      "Gradient Descent(998/999): loss=4.516237864890023, w0=-3.240220433756454, w1=1.1271754867888548\n",
      "Gradient Descent(999/999): loss=4.516096429262984, w0=-3.2414021442744225, w1=1.1272942024281842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.24140214,  1.1272942 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.515955503078913"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
