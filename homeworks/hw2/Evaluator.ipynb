{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baixianger/DLAV-2022/blob/main/homeworks/hw2/Evaluator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG9XfcBSihp9"
      },
      "source": [
        "This is a GPU version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPFXjAVFIKnh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import platform\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGxnwhvlwMiI",
        "outputId": "e30466af-f9da-44ee-b8a5-038061911a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwxcJW9wI9fp",
        "outputId": "e64456b7-4350-40c9-ca18-0093ed052f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DLAV-2022'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 68 (delta 22), reused 54 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (68/68), 27.40 MiB | 12.22 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vita-epfl/DLAV-2022.git\n",
        "path = os.getcwd() + '/DLAV-2022/homeworks/hw2/test_batch'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pZXQTJIKJE_S"
      },
      "outputs": [],
      "source": [
        "# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n",
        "### Path to Model Weights \n",
        "softmax_weights = f'./softmax_weights.pkl'\n",
        "# A GPU backbone\n",
        "pytorch_weights = f'./linearClassifier_pytorch_CUDA.ckpt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6psT_aVPHv"
      },
      "source": [
        "**TODO:** Copy your code from the Softmax Notebook to their corresponding function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gHnLX6-oIkWm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def softmax_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    D, C = W.shape\n",
        "    N = X.shape[0]\n",
        "    Y_onehot = np.eye(C)[y]\n",
        "    idx = (Y_onehot == 1.)\n",
        "    Z = X @ W                                                            # (N, C)\n",
        "    Y_hat = np.exp(Z) / np.exp(Z).sum(axis=1, keepdims=True)             # (N, C)\n",
        "    loss = - np.log(Y_hat[idx]).sum() / N                                # (1,)\n",
        "    # Alternatively we can use below method to calculate loss\n",
        "    # loss = - Y_onehot * np.log(Y_hat)).sum()/N\n",
        "    dLdZ = Y_hat - Y_onehot                                              # (N, C)\n",
        "    dLdW = X.T @ dLdZ / N\n",
        "    dW = dLdW\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "    \n",
        "    return loss, dW\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n",
        "                batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        \n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "        \n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            idx = np.random.choice(range(num_train),batch_size) \n",
        "            X_batch = X[idx,:]\n",
        "            y_batch = y[idx]\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W = self.W - learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "\n",
        "        return loss_history\n",
        "    \n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        N = X.shape[0]\n",
        "        Z = X @ self.W                                                     # (N, C)\n",
        "        y_hat = np.exp(Z) / np.exp(Z).sum(axis=1, keepdims=True)           # (N, C)\n",
        "        y_pred = y_hat.argmax(axis=1)\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative. \n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \n",
        "         e = y_batch - np.dot(X_batch, self.W) \n",
        "        \n",
        "        loss = np.dot(e.T, e)\n",
        "        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n",
        "  \n",
        "        return loss, grad\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "        \n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6chaH4G-Vfms"
      },
      "source": [
        "**TODO:** Copy the model you created from the Pytorch Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mSTfKTHEJBhy"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)\n",
        "        self.fc3 = torch.nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UUbNTUAVsos"
      },
      "source": [
        "**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bEKafMuaI4By"
      },
      "outputs": [],
      "source": [
        "def predict_usingPytorch(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Create your model                                                   #\n",
        "    # - Load your saved model                                               #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n",
        "    #########################################################################\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    net = Net(n_feature=3*32*32, n_hidden=1024, n_output=10).to(device)\n",
        "    checkpoint = torch.load(pytorch_weights, map_location=torch.device(device))\n",
        "    net.load_state_dict(checkpoint)\n",
        "    outputs = net(X.to(device))\n",
        "    _, y_pred = torch.max(F.softmax(outputs,dim=1).data, 1)\n",
        "    y_pred = y_pred.to('cpu')\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred.numpy()\n",
        "\n",
        "def predict_usingSoftmax(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Load your saved model into the weights of Softmax                   #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array                                 #\n",
        "    #########################################################################\n",
        "    with open(softmax_weights, 'rb') as f:\n",
        "        W = pickle.load(f)\n",
        "    my_softmax = Softmax()\n",
        "    my_softmax.W = W.copy()\n",
        "    y_pred = my_softmax.predict(X)\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8dM8fj39OBP"
      },
      "source": [
        "This method loads the test dataset to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "400u4eZNJAZq"
      },
      "outputs": [],
      "source": [
        "## Read DATA\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "test_filename = path\n",
        "X,Y = load_CIFAR_batch(test_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ3mBYnx9TIe"
      },
      "source": [
        "This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IEmU5KnwJPBY"
      },
      "outputs": [],
      "source": [
        "## Data Manipulation\n",
        "\n",
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std = np.array([0.2023, 0.1994, 0.2010])\n",
        "X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "\n",
        "X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n",
        "X_softmax = np.reshape(X, (X.shape[0], -1))\n",
        "X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2nQbKPL9c3G"
      },
      "source": [
        "Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKFPhm1wJjDv",
        "outputId": "aed41e1f-6ef6-4753-90f3-72f92990d0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Softmax= 0.408500 ... Pytorch= 0.673600\n"
          ]
        }
      ],
      "source": [
        "## Run Prediction\n",
        "prediction_pytorch = predict_usingPytorch(X_pytorch)\n",
        "prediction_softmax = predict_usingSoftmax(X_softmax)\n",
        "\n",
        "## Run Evaluation\n",
        "acc_softmax = sum(prediction_softmax == Y)/len(X)\n",
        "acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n",
        "print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Evaluator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
