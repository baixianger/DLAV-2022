{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"aEW8FsaTRaOo"},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import torch.utils.data as utils\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","import platform\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJAZFldDTQRz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dtBf2_UXTPVs"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm: DLAV-2022: No such file or directory\n","Cloning into 'DLAV-2022'...\n","remote: Enumerating objects: 83, done.\u001b[K\n","remote: Counting objects: 100% (83/83), done.\u001b[K\n","remote: Compressing objects: 100% (63/63), done.\u001b[K\n","remote: Total 83 (delta 31), reused 60 (delta 16), pack-reused 0\u001b[K\n","Receiving objects: 100% (83/83), 27.73 MiB | 3.60 MiB/s, done.\n","Resolving deltas: 100% (31/31), done.\n"]}],"source":["!rm -r DLAV-2022\n","!git clone https://github.com/vita-epfl/DLAV-2022.git\n","path = os.getcwd() + '/DLAV-2022/homeworks/hw2/test_batch'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QB1xSP9nUZGQ"},"outputs":[],"source":["# Set the variable to the location of the trained model\n","model_path = './cifar10_vgg11bn_CUDA.ckpt'"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"QEIcPZ4hT7fY"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, n_input_channels=3, n_output=10):\n","        super().__init__()\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Define 2 or more different layers of the neural network                      #\n","        ################################################################################\n","        # use the VGG11bn model which 8 convolution layers and 3 full connect layers\n","        # '?' denotes the batch size, which is 64 in our default setting from func above\n","        # network:[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","        self.features = nn.Sequential(\n","                # 1. 64\n","                nn.Conv2d(3, 64, kernel_size=3, padding=1),         # (  3,32,32) => ( 64,32,32)\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2),              # ( 64,32,32) => ( 64,16,16)\n","                # 2. 128 conv\n","                nn.Conv2d(64, 128, kernel_size=3, padding=1),       # ( 64,16,16) => (128,16,16)\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2),              # (128,16,16) => (128, 8, 8)\n","                # 3. 256 conv\n","                nn.Conv2d(128, 256, kernel_size=3, padding=1),      # (128, 8, 8) => (256, 8, 8)\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                # 4. 256\n","                nn.Conv2d(256, 256, kernel_size=3, padding=1),      # (256, 8, 8) => (256, 8, 8)\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2),              # (256, 8, 8) => (256, 4, 4)\n","                # 5. 512\n","                nn.Conv2d(256, 512, kernel_size=3, padding=1),      # (512, 4, 4) => (512, 4, 4)\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),                \n","                # 6. 512\n","                nn.Conv2d(512, 512, kernel_size=3, padding=1),      # (512, 4, 4) => (512, 4, 4)\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),  \n","                nn.MaxPool2d(kernel_size=2, stride=2),              # (512, 4, 4) => (512, 2, 2)\n","                # 7. 512\n","                nn.Conv2d(512, 512, kernel_size=3, padding=1),      # (512, 2, 2) => (512, 2, 2)\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),  \n","                # 8. 512\n","                nn.Conv2d(512, 512, kernel_size=3, padding=1),      # (512, 2, 2) => (512, 2, 2)\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),  \n","                nn.MaxPool2d(kernel_size=2, stride=2),              # (512, 2, 2) => (512, 1, 1)\n","        )\n","        self.classifier = nn.Sequential(\n","                # 9. Linear\n","                nn.Dropout(),\n","                nn.Linear(512, 512),                                # (512, 1, 1) => flatten => (512,)\n","                nn.ReLU(True),\n","                # 10. linear\n","                nn.Dropout(),\n","                nn.Linear(512, 512),                                # (512,) => (512,)\n","                nn.ReLU(True),\n","                # 11. linear\n","                nn.Linear(512, 10),                                 # (512,) => (10,)\n","        )\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","    \n","    def forward(self, x):\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Set up the forward pass that the input data will go through.                 #\n","        # A good activation function betweent the layers is a ReLu function.           #\n","        #                                                                              #\n","        # Note that the output of the last convolution layer should be flattened       #\n","        # before being inputted to the fully connected layer. We can flatten           #\n","        # Tensor `x` with `x.view`.                                                    #\n","        ################################################################################\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","        \n","        return x\n","    \n","    def predict(self, x):\n","        logits = self.forward(x)\n","        return F.softmax(logits)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"p5v7OgpDR4m2"},"outputs":[],"source":["def predict_usingCNN(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Load your saved model                                               #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array                                 #\n","    # Note: For the predictions, you have to return the index of the max    #\n","    # value                                                                 #\n","    #########################################################################\n","    # CUDA setting\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    net = ConvNet().to(device)\n","    # Load my optimized model\n","    checkpoint = torch.load(model_path, map_location=torch.device(device))\n","\n","    net.load_state_dict(checkpoint)\n","    # prediction/inference\n","    with torch.no_grad():\n","        X = X.to(device)\n","        outputs = net(X)\n","        _, predicted = torch.max(outputs.data, 1)\n","        # _, predicted = torch.max(F.softmax(outputs,dim=1).data, 1)\n","        # cannot turn a CUDA tensor to numpy, so move to CPU first\n","        y_pred = predicted.to('cpu').numpy()\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred\n","   "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"IGqVw4U3Sy21"},"outputs":[],"source":["## Read DATA\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","test_filename = path\n","X,Y = load_CIFAR_batch(test_filename)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"qiRBbv7fR-DB"},"outputs":[{"name":"stdout","output_type":"stream","text":["CNN Accuracy= 0.966100\n"]}],"source":["# Data Manipulation\n","mean_pytorch = np.array([0.4914, 0.4822, 0.4465])\n","std_pytorch = np.array([0.2023, 0.1994, 0.2010])\n","X_pytorch = np.divide(np.subtract( X/255 , mean_pytorch[np.newaxis, :,np.newaxis,np.newaxis]), std_pytorch[np.newaxis, :,np.newaxis,np.newaxis])\n","\n","# Run Prediction and Evaluation\n","prediction_cnn = predict_usingCNN(torch.from_numpy(X_pytorch).float())\n","acc_cnn = sum(prediction_cnn == Y)/len(X_pytorch)\n","print(\"CNN Accuracy= %f\"%(acc_cnn))"]},{"cell_type":"markdown","metadata":{},"source":["I found that the accuracy on the final test data is a little higher than the accuracy when i was tranning on the validation data. \n","\n","I check the code in the `CNN_Exercise.ipynb`, and i suppose is the way we calculate the validation accuracy, because the last bach of every epoch is usually not the normal size (The number of samples in the data set is not divisible by the batch size) but our way to calulate the accuracy is using accumulated batch accuracy dividied by its batch size in which the last batch in one epoch will be smaller than the normal batch size, and than accumulating the averaged batch accuracy , and lastly divided by its step. \n","\n","So the last batch in every epoch will heavily affect (`lead to underestimation`) the calculation of final accuracy in our validation."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNV/YURRLTgyD62qxCvzW0s","collapsed_sections":[],"name":"Evaluator.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
